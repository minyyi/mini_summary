# ============== PDF ë¶„ì„ Streamlit ì•± ==============
import streamlit as st
import fitz  # pymupdf
import faiss
import numpy as np
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from typing import TypedDict, List, Dict, Any
import pickle
import os
import tempfile
from dotenv import load_dotenv
import time

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# í™˜ê²½ ì„¤ì •
load_dotenv()

# ============== ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ==============
def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”"""
    if 'chunks' not in st.session_state:
        st.session_state.chunks = []
    if 'index' not in st.session_state:
        st.session_state.index = None
    if 'embedding_model' not in st.session_state:
        st.session_state.embedding_model = None
    if 'metadatas' not in st.session_state:
        st.session_state.metadatas = []
    if 'file_processed' not in st.session_state:
        st.session_state.file_processed = False
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'agent_graph' not in st.session_state:
        st.session_state.agent_graph = None
    if 'uploaded_filename' not in st.session_state:
        st.session_state.uploaded_filename = ""

# ì´ˆê¸°í™” ì‹¤í–‰
init_session_state()

# ============== íƒ€ì… ì •ì˜ ==============
class AgentState(TypedDict):
    messages: List[Any]

# ============== íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ==============

def extract_text_from_file(uploaded_file):
    """ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    file_type = uploaded_file.type
    file_name = uploaded_file.name.lower()
    
    try:
        if file_type == "application/pdf" or file_name.endswith('.pdf'):
            return extract_text_with_fitz(uploaded_file)
        
        elif file_type == "text/plain" or file_name.endswith(('.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv')):
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
            content = uploaded_file.read()
            if isinstance(content, bytes):
                # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
                for encoding in ['utf-8', 'cp949', 'euc-kr', 'latin-1']:
                    try:
                        text = content.decode(encoding)
                        st.success(f"âœ… í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì™„ë£Œ! ({encoding} ì¸ì½”ë”©)")
                        return text
                    except UnicodeDecodeError:
                        continue
                st.error("âŒ í…ìŠ¤íŠ¸ íŒŒì¼ ì¸ì½”ë”©ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ""
            else:
                return str(content)
        
        else:
            st.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_type}")
            return ""
            
    except Exception as e:
        st.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""

def extract_text_with_fitz(pdf_file):
    """PyMuPDFë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(pdf_file.read())
            tmp_path = tmp_file.name
        
        doc = fitz.open(tmp_path)
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        full_text = ""
        total_pages = len(doc)
        
        for page_num in range(total_pages):
            progress = (page_num + 1) / total_pages
            progress_bar.progress(progress)
            status_text.text(f"í˜ì´ì§€ {page_num + 1}/{total_pages} ì²˜ë¦¬ ì¤‘...")
            
            page = doc[page_num]
            text = page.get_text()
            
            if text and text.strip():
                text = text.replace('\x00', '')
                text = text.replace('\ufeff', '')
                text = text.replace('\r\n', '\n')
                text = text.replace('\r', '\n')
                full_text += text + "\n\n"
        
        doc.close()
        os.unlink(tmp_path)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        
        progress_bar.progress(1.0)
        status_text.text(f"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ! ì´ {len(full_text)} ê¸€ì")
        time.sleep(1)
        progress_bar.empty()
        status_text.empty()
        
        return full_text
        
    except Exception as e:
        st.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""

def chunk_text(text, chunk_size=2000, overlap=100):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜"""
    if not text.strip():
        return []
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end].strip()
        
        if chunk:
            chunks.append(chunk)
        
        start = end - overlap
    
    return chunks

def create_vectorstore(chunks):
    """FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
    if not chunks:
        return None, None, None
    
    try:
        with st.spinner('ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)'):
            embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')
            embeddings = embedding_model.embed_documents(chunks)
            embeddings = np.array(embeddings).astype('float32')
            
            # FAISS ì¸ë±ìŠ¤ ìƒì„±
            dimension = embeddings.shape[1]
            index = faiss.IndexFlatL2(dimension)
            index.add(embeddings)
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadatas = []
            for i, chunk in enumerate(chunks):
                metadatas.append({
                    'chunk_id': i,
                    'chunk_size': len(chunk),
                    'preview': chunk[:100] + "..." if len(chunk) > 100 else chunk
                })
        
        st.success(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! {index.ntotal}ê°œ ë²¡í„° ì €ì¥")
        return index, embedding_model, metadatas
        
    except Exception as e:
        st.error(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì˜¤ë¥˜: {e}")
        return None, None, None

def safe_search_documents(query, k=3):
    """ì•ˆì „í•œ ë¬¸ì„œ ê²€ìƒ‰"""
    try:
        if (not hasattr(st.session_state, 'index') or 
            st.session_state.index is None or 
            not hasattr(st.session_state, 'embedding_model') or
            st.session_state.embedding_model is None):
            return []
        
        query_embedding = st.session_state.embedding_model.embed_query(query)
        query_embedding = np.array([query_embedding]).astype('float32')
        
        distances, indices = st.session_state.index.search(query_embedding, k)
        
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(st.session_state.chunks):
                results.append({
                    'chunk_id': idx,
                    'content': st.session_state.chunks[idx],
                    'score': 1.0 - (distance / 2.0),
                    'metadata': st.session_state.metadatas[idx] if idx < len(st.session_state.metadatas) else {}
                })
        
        return results
        
    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# ============== ì§ì ‘ í˜¸ì¶œ í•¨ìˆ˜ë“¤ (tool ë°ì½”ë ˆì´í„° ì œê±°) ==============

def search_uploaded_documents_direct(query: str, k: int = 3) -> str:
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        if not hasattr(st.session_state, 'file_processed') or not st.session_state.file_processed:
            return "ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”."
        
        results = safe_search_documents(query, k)
        
        if not results:
            return f"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_results = []
        for i, result in enumerate(results, 1):
            formatted_results.append(
                # f"[ê²€ìƒ‰ê²°ê³¼ {i}] (ìœ ì‚¬ë„: {result['score']:.3f})\n"
                f"{result['content'][:400]}{'...' if len(result['content']) > 400 else ''}\n"
            )
        
        return "\n".join(formatted_results)
        
    except Exception as e:
        return f"íŒŒì¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"

def web_search_tool_direct(query: str) -> str:
    """ì›¹ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        from langchain_community.tools import DuckDuckGoSearchRun
        search_tool = DuckDuckGoSearchRun()
        results = search_tool.run(query)
        return f"ğŸŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n\n{results}"
    except ImportError:
        return f"""ğŸ” "{query}" ê²€ìƒ‰ ë§í¬:
â€¢ Google: https://www.google.com/search?q={query.replace(' ', '+')}
â€¢ ë„¤ì´ë²„: https://search.naver.com/search.naver?query={query.replace(' ', '+')}

ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰: pip install duckduckgo-search"""
    except Exception as e:
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"

def general_chat_tool_direct(question: str) -> str:
    """ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."""
    try:
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
        response = llm.invoke([HumanMessage(content=question)])
        return response.content
    except Exception as e:
        return f"ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"

def summarize_content_tool_direct(content: str = "", style: str = "general") -> str:
    """ì£¼ì–´ì§„ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤."""
    try:
        # contentê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—…ë¡œë“œëœ íŒŒì¼ ì‚¬ìš©
        if not content.strip():
            if (not hasattr(st.session_state, 'file_processed') or 
                not st.session_state.file_processed or
                not hasattr(st.session_state, 'chunks') or
                not st.session_state.chunks):
                return "ìš”ì•½í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”."
            
            # ì²˜ìŒ 5ê°œ ì²­í¬ë¥¼ ìš”ì•½ (ë” ë§ì€ ë‚´ìš©)
            content = "\n".join(st.session_state.chunks[:5])
        
        if not content.strip():
            return "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ì²˜ë¦¬
        if len(content) > 8000:
            content = content[:8000] + "..."
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
        
        # ê°„ë‹¨í•œ ìš”ì•½ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¤ìŒ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{content}

ìš”ì•½:"""
        
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
        
    except Exception as e:
        return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def get_document_info_tool_direct() -> str:
    """í˜„ì¬ ë¡œë“œëœ íŒŒì¼ì˜ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        if not hasattr(st.session_state, 'file_processed') or not st.session_state.file_processed:
            return "ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”."
        
        if not hasattr(st.session_state, 'chunks') or not st.session_state.chunks:
            return "íŒŒì¼ ì²˜ë¦¬ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        total_chars = sum(len(chunk) for chunk in st.session_state.chunks)
        filename = getattr(st.session_state, 'uploaded_filename', 'ì•Œ ìˆ˜ ì—†ìŒ')
        
        return f"""ğŸ“„ í˜„ì¬ ë¡œë“œëœ íŒŒì¼ ì •ë³´:
- íŒŒì¼ëª…: {filename}
- ì´ ì²­í¬ ìˆ˜: {len(st.session_state.chunks)}
- ì´ ê¸€ì ìˆ˜: {total_chars:,}
- í‰ê·  ì²­í¬ í¬ê¸°: {total_chars // len(st.session_state.chunks):,} ê¸€ì

ğŸ› ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥ë“¤:
- íŒŒì¼ ë‚´ìš© ê²€ìƒ‰
- ì›¹ì—ì„œ ìµœì‹  ì •ë³´ ê²€ìƒ‰
- ë‚´ìš© ìš”ì•½
- íŒŒì¼ ì •ë³´ ì¡°íšŒ

ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸: "íŒŒì¼ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ì°¾ì•„ì¤˜", "íŒŒì¼ì„ ìš”ì•½í•´ì¤˜" """
    
    except Exception as e:
        return f"íŒŒì¼ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"

# ============== LangGraph ê·¸ë˜í”„ ==============

def should_continue(state: AgentState) -> str:
    """ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜"""
    messages = state["messages"]
    last_message = messages[-1]
    
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tools"
    
    return END

def call_model(state: AgentState) -> AgentState:
    """LLM ëª¨ë¸ í˜¸ì¶œ"""
    messages = state["messages"]
    
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì‚¬ìš©ì ë©”ì‹œì§€ì¸ì§€ í™•ì¸
    if not messages or not isinstance(messages[-1], HumanMessage):
        return state
    
    user_message = messages[-1].content
    
    try:
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¼ìš°íŒ…
        user_lower = user_message.lower()
        
        # íŒŒì¼ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
        file_keywords = ['íŒŒì¼', 'ë¬¸ì„œ', 'ìš”ì•½', 'ê²€ìƒ‰', 'ì •ë³´', 'ë‚´ìš©']
        is_file_related = any(keyword in user_lower for keyword in file_keywords)
        
        # ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ ê°ì§€
        web_keywords = ['ê²€ìƒ‰', 'ë‰´ìŠ¤', 'ìµœì‹ ', 'ë‚ ì”¨', 'í˜„ì¬']
        needs_web_search = any(keyword in user_lower for keyword in web_keywords)
        
        response_content = ""
        
        if is_file_related and st.session_state.file_processed:
            # íŒŒì¼ ê´€ë ¨ ì²˜ë¦¬
            if 'ìš”ì•½' in user_lower:
                response_content = summarize_content_tool_direct()
            elif 'ì •ë³´' in user_lower:
                response_content = get_document_info_tool_direct()
            else:
                response_content = search_uploaded_documents_direct(user_message)
        
        elif needs_web_search:
            # ì›¹ ê²€ìƒ‰ ì²˜ë¦¬
            response_content = web_search_tool_direct(user_message)
        
        else:
            # ì¼ë°˜ ì±„íŒ… ì²˜ë¦¬
            response_content = general_chat_tool_direct(user_message)
        
        # AI ì‘ë‹µ ìƒì„±
        ai_response = AIMessage(content=response_content)
        return {"messages": messages + [ai_response]}
        
    except Exception as e:
        error_response = AIMessage(content=f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return {"messages": messages + [error_response]}

def create_agent_graph():
    """ê°„ë‹¨í•œ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ìƒì„±"""
    graph_builder = StateGraph(AgentState)
    graph_builder.add_node("agent", call_model)
    graph_builder.set_entry_point("agent")
    graph_builder.add_edge("agent", END)
    
    return graph_builder.compile()

# ============== Streamlit UI ==============

def main():
    st.title("ğŸ¤– íŒŒì¼ ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸")
    st.markdown("---")
    
    # API í‚¤ ì²´í¬
    if not os.getenv('OPENAI_API_KEY'):
        st.error("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“„ íŒŒì¼ ì—…ë¡œë“œ")
        
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_file = st.file_uploader(
            "íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type=['pdf', 'txt', 'md', 'py', 'js', 'html', 'css', 'json', 'xml', 'csv'],
            help="PDF, í…ìŠ¤íŠ¸ íŒŒì¼, ì½”ë“œ íŒŒì¼ ë“±ì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        
        if uploaded_file is not None:
            st.info(f"ğŸ“ ì„ íƒëœ íŒŒì¼: {uploaded_file.name}")
            st.info(f"ğŸ“ íŒŒì¼ í¬ê¸°: {uploaded_file.size:,} bytes")
            
            if st.button("ğŸ“Š íŒŒì¼ ë¶„ì„ ì‹œì‘", type="primary"):
                with st.spinner("íŒŒì¼ì„ ë¶„ì„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    try:
                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        full_text = extract_text_from_file(uploaded_file)
                        
                        if full_text:
                            # ì²­í‚¹
                            st.session_state.chunks = chunk_text(full_text)
                            st.info(f"âœ… ì²­í‚¹ ì™„ë£Œ! ì´ {len(st.session_state.chunks)}ê°œ ì²­í¬ ìƒì„±")
                            
                            # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                            index, embedding_model, metadatas = create_vectorstore(st.session_state.chunks)
                            
                            if index is not None:
                                st.session_state.index = index
                                st.session_state.embedding_model = embedding_model
                                st.session_state.metadatas = metadatas
                                st.session_state.file_processed = True
                                st.session_state.uploaded_filename = uploaded_file.name
                                
                                st.success("ğŸ‰ íŒŒì¼ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                                st.balloons()
                                
                                # st.snow()
                    except Exception as e:
                        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ë¬¸ì„œ ì •ë³´ í‘œì‹œ
        if st.session_state.file_processed:
            st.markdown("---")
            st.header("ğŸ“Š íŒŒì¼ ì •ë³´")
            
            if hasattr(st.session_state, 'uploaded_filename'):
                st.info(f"ğŸ“ íŒŒì¼ëª…: {st.session_state.uploaded_filename}")
            
            if st.session_state.chunks:
                total_chars = sum(len(chunk) for chunk in st.session_state.chunks)
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("ì²­í¬ ìˆ˜", len(st.session_state.chunks))
                with col2:
                    st.metric("ì´ ê¸€ì ìˆ˜", f"{total_chars:,}")
            
            # ì´ˆê¸°í™” ë²„íŠ¼
            if st.button("ğŸ”„ ì´ˆê¸°í™”", help="ì—…ë¡œë“œëœ íŒŒì¼ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤"):
                for key in ['chunks', 'index', 'embedding_model', 'metadatas', 'file_processed', 'chat_history', 'uploaded_filename']:
                    if key in st.session_state:
                        del st.session_state[key]
                st.rerun()
    
    # ë©”ì¸ ì½˜í…ì¸  - í•­ìƒ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ í‘œì‹œ
    st.header("ğŸ’¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì™€ ëŒ€í™”í•˜ê¸°")
    
    if not st.session_state.file_processed:
        st.info("ğŸ’¡ ì¼ë°˜ ì§ˆë¬¸ë„ ê°€ëŠ¥í•˜ê³ , íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ íŒŒì¼ ë¶„ì„ë„ í•  ìˆ˜ ìˆì–´ìš”!")
        
        # ì‚¬ìš©ë²• ì•ˆë‚´
        with st.expander("ğŸ“– ì‚¬ìš©ë²• ì•ˆë‚´", expanded=False):
            st.markdown("""
            ### ğŸš€ ë‘ ê°€ì§€ ì‚¬ìš© ë°©ë²•
            
            #### 1ï¸âƒ£ ì¼ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œ ì‚¬ìš©
            - ë°”ë¡œ ì•„ë˜ì—ì„œ ì§ˆë¬¸í•˜ì„¸ìš”!
            - ì›¹ ê²€ìƒ‰, ì¼ë°˜ ì§€ì‹ ì§ˆë¬¸ ë“± ëª¨ë“  ì§ˆë¬¸ ê°€ëŠ¥
            
            #### 2ï¸âƒ£ íŒŒì¼ ë¶„ì„ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
            1. **íŒŒì¼ ì—…ë¡œë“œ**: ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”
               - ğŸ“„ **PDF**: ë¬¸ì„œ, ë…¼ë¬¸, ë³´ê³ ì„œ ë“±
               - ğŸ“ **í…ìŠ¤íŠ¸**: .txt, .md íŒŒì¼
               - ğŸ’» **ì½”ë“œ**: .py, .js, .html, .css, .json ë“±
               - ğŸ“Š **ë°ì´í„°**: .csv, .xml ë“±
            2. **ë¶„ì„ ì‹œì‘**: "íŒŒì¼ ë¶„ì„ ì‹œì‘" ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
            3. **íŒŒì¼ ì§ˆë¬¸**: ì—…ë¡œë“œí•œ íŒŒì¼ì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
            
            ### ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸
            **ì¼ë°˜ ì§ˆë¬¸:**
            - "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?"
            - "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?"
            - "ìµœì‹  AI ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì¤˜"
            
            **íŒŒì¼ ì§ˆë¬¸ (ì—…ë¡œë“œ í›„):**
            - "ì´ íŒŒì¼ì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜"
            - "ì½”ë“œì—ì„œ í•¨ìˆ˜ë“¤ì„ ì„¤ëª…í•´ì¤˜"
            - "íŒŒì¼ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì°¾ì•„ì¤˜"
            - "ì´ ë°ì´í„°ì˜ íŒ¨í„´ì„ ë¶„ì„í•´ì¤˜"
            """)
    
    # ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì´ˆê¸°í™” (ëª¨ë“  tool í•¨ìˆ˜ê°€ ì •ì˜ëœ í›„ì— ì‹¤í–‰)
    try:
        if st.session_state.agent_graph is None:
            st.session_state.agent_graph = create_agent_graph()
    except Exception as e:
        st.error(f"AI ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
        st.info("í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    chat_container = st.container()
    
    with chat_container:
        for i, (role, message) in enumerate(st.session_state.chat_history):
            if role == "user":
                with st.chat_message("user"):
                    st.write(message)
            else:
                with st.chat_message("assistant"):
                    st.write(message)
    
    # ì‚¬ìš©ì ì…ë ¥ (í•­ìƒ ë§¨ ì•„ë˜ì— ê³ ì •)
    st.markdown("---")
    st.subheader("âœï¸ ì§ˆë¬¸í•˜ê¸°")
    
    # ì…ë ¥ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜
    def process_user_input():
        user_input = st.session_state.get("user_input", "").strip()
        if user_input:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.chat_history.append(("user", user_input))
            
            # AI ì‘ë‹µ ìƒì„±
            try:
                # ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì‚¬ìš©
                if st.session_state.agent_graph is None:
                    st.session_state.agent_graph = create_agent_graph()
                
                initial_state = {"messages": [HumanMessage(content=user_input)]}
                result = st.session_state.agent_graph.invoke(initial_state)
                
                # ì‘ë‹µ ì¶”ì¶œ
                if result and "messages" in result and len(result["messages"]) > 1:
                    final_message = result["messages"][-1]
                    response = final_message.content if hasattr(final_message, 'content') else str(final_message)
                else:
                    response = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                st.session_state.chat_history.append(("assistant", response))
                
            except Exception as e:
                error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.session_state.chat_history.append(("assistant", error_msg))
            
            # ì…ë ¥ì°½ í´ë¦¬ì–´ ë° í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
            st.session_state.user_input = ""
            st.rerun()
    
    # í…ìŠ¤íŠ¸ ì…ë ¥ê³¼ ë²„íŠ¼ì„ ê°™ì€ í–‰ì— ë°°ì¹˜
    col1, col2 = st.columns([4, 1])
    
    with col1:
        user_input = st.text_input(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: 'íŒŒì´ì¬ì´ë€?' ë˜ëŠ” 'íŒŒì¼ì„ ìš”ì•½í•´ì¤˜' (Enter ë˜ëŠ” ì „ì†¡ ë²„íŠ¼)",
            key="user_input",
            on_change=process_user_input  # ì—”í„°í‚¤ ì²˜ë¦¬
        )
    
    with col2:
        send_button = st.button("ì „ì†¡", type="primary", use_container_width=True)
    
    # ì „ì†¡ ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬
    # if send_button:
    #     process_user_input()
    if send_button and user_input.strip():
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_history.append(("user", user_input))
        
        # ì¦‰ì‹œ UI ì—…ë°ì´íŠ¸
        with st.chat_message("user"):
            st.write(user_input)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì‚¬ìš©
                    if st.session_state.agent_graph is None:
                        st.session_state.agent_graph = create_agent_graph()
                    
                    initial_state = {"messages": [HumanMessage(content=user_input)]}
                    result = st.session_state.agent_graph.invoke(initial_state)
                    
                    # ì‘ë‹µ ì¶”ì¶œ
                    if result and "messages" in result and len(result["messages"]) > 1:
                        final_message = result["messages"][-1]
                        response = final_message.content if hasattr(final_message, 'content') else str(final_message)
                    else:
                        response = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    
                    st.write(response)
                    
                    # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                    st.session_state.chat_history.append(("assistant", response))
                    
                except Exception as e:
                    error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    st.error(error_msg)
                    st.session_state.chat_history.append(("assistant", error_msg))
    
    # ë¹ ë¥¸ ì§ˆë¬¸ ë²„íŠ¼ë“¤
    st.markdown("---")
    
    if st.session_state.file_processed:
        st.subheader("ğŸ¯ íŒŒì¼ ê´€ë ¨ ë¹ ë¥¸ ì§ˆë¬¸")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ğŸ“ íŒŒì¼ ìš”ì•½"):
                # ì§ì ‘ ì‘ë‹µ ìƒì„±
                try:
                    response = summarize_content_tool_direct()
                    st.session_state.chat_history.append(("user", "ì´ íŒŒì¼ì„ ìš”ì•½í•´ì¤˜"))
                    st.session_state.chat_history.append(("assistant", response))
                    st.rerun()
                except Exception as e:
                    st.error(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {e}")
        
        with col2:
            if st.button("ğŸ“Š íŒŒì¼ ì •ë³´"):
                # ì§ì ‘ ì‘ë‹µ ìƒì„±
                try:
                    response = get_document_info_tool_direct()
                    st.session_state.chat_history.append(("user", "íŒŒì¼ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜"))
                    st.session_state.chat_history.append(("assistant", response))
                    st.rerun()
                except Exception as e:
                    st.error(f"ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        
        with col3:
            if st.button("ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰"):
                try:
                    response = search_uploaded_documents_direct("ì¤‘ìš”í•œ í‚¤ì›Œë“œ")
                    st.session_state.chat_history.append(("user", "íŒŒì¼ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì°¾ì•„ì¤˜"))
                    st.session_state.chat_history.append(("assistant", response))
                    st.rerun()
                except Exception as e:
                    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        with col4:
            if st.button("ğŸ“„ íŒŒì¼ ë‚´ ê²€ìƒ‰"):
                try:
                    response = "ì–´ë–¤ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë‚˜ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
                    st.session_state.chat_history.append(("user", "íŒŒì¼ì—ì„œ íŠ¹ì • ë‚´ìš©ì„ ê²€ìƒ‰í•˜ê³  ì‹¶ì–´"))
                    st.session_state.chat_history.append(("assistant", response))
                    st.rerun()
                except Exception as e:
                    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    else:
        st.subheader("ğŸ¯ ì¼ë°˜ ì§ˆë¬¸ ì˜ˆì‹œ")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ğŸ íŒŒì´ì¬ ì§ˆë¬¸"):
                # ì§ì ‘ ì‘ë‹µ ìƒì„±
                try:
                    response = general_chat_tool_direct("íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?")
                    st.session_state.chat_history.append(("user", "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?"))
                    st.session_state.chat_history.append(("assistant", response))
                    st.rerun()
                except Exception as e:
                    st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        
        with col2:
            if st.button("ğŸŒ ì›¹ ê²€ìƒ‰"):
                # ì§ì ‘ ì‘ë‹µ ìƒì„±
                try:
                    response = web_search_tool_direct("ìµœì‹  AI ë‰´ìŠ¤")
                    st.session_state.chat_history.append(("user", "ìµœì‹  AI ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì¤˜"))
                    st.session_state.chat_history.append(("assistant", response))
                    st.rerun()
                except Exception as e:
                    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        with col3:
            if st.button("ğŸ¤– AI ì§ˆë¬¸"):
                # ì§ì ‘ ì‘ë‹µ ìƒì„±
                try:
                    response = general_chat_tool_direct("ChatGPTì™€ Claudeì˜ ì°¨ì´ì ì€?")
                    st.session_state.chat_history.append(("user", "ChatGPTì™€ Claudeì˜ ì°¨ì´ì ì€?"))
                    st.session_state.chat_history.append(("assistant", response))
                    st.rerun()
                except Exception as e:
                    st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        
        with col4:
            if st.button("ğŸ’» ì½”ë”© ì§ˆë¬¸"):
                # ì§ì ‘ ì‘ë‹µ ìƒì„±
                try:
                    response = general_chat_tool_direct("íŒŒì´ì¬ í´ë˜ìŠ¤ì™€ ê°ì²´ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì¤˜")
                    st.session_state.chat_history.append(("user", "íŒŒì´ì¬ í´ë˜ìŠ¤ì™€ ê°ì²´ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì¤˜"))
                    st.session_state.chat_history.append(("assistant", response))
                    st.rerun()
                except Exception as e:
                    st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
                    try:
                        response = general_chat_tool("íŒŒì´ì¬ í´ë˜ìŠ¤ì™€ ê°ì²´ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì¤˜")
                        st.session_state.chat_history.append(("user", "íŒŒì´ì¬ í´ë˜ìŠ¤ì™€ ê°ì²´ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì¤˜"))
                        st.session_state.chat_history.append(("assistant", response))
                        st.rerun()
                    except Exception as e:
                        st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()