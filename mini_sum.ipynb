{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9baeea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0f93176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (1.46.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-openai in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (0.3.24)\n",
      "Requirement already satisfied: faiss-cpu in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: pymupdf in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (1.26.1)\n",
      "Requirement already satisfied: langchain-community in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: numpy in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (6.1.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: packaging<26,>=20 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (2.3.0)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (11.2.1)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (6.31.1)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (20.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (2.32.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (4.14.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-openai) (0.3.65)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-openai) (1.88.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-community) (3.12.13)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-community) (0.3.45)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: jinja2 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.43.1)\n",
      "Requirement already satisfied: colorama in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\ai_prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit python-dotenv langchain-openai faiss-cpu pymupdf langchain-community numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b1600bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f721b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a99bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 파일: 2024-PR-15.pdf\n",
      "총 페이지 수: 127\n",
      "페이지 1 처리 중...\n",
      "  페이지 1: 61 글자 추출\n",
      "  첫 50글자: 약자와 동행하는 서울의 교통\n",
      "이신해   김승준   한영준   양재환 \n",
      "윤서연   연준형  \n",
      "페이지 2 처리 중...\n",
      "  페이지 2: 16 글자 추출\n",
      "  첫 50글자: 약자와 동행하는 서울의 교통\n",
      "\n",
      "페이지 3 처리 중...\n",
      "  페이지 3: 257 글자 추출\n",
      "  첫 50글자: 이 보고서의 내용은 연구진의 견해로서 \n",
      "서울특별시의 정책과는 다를 수도 있습니다.\n",
      "연구책임\n",
      "페이지 4 처리 중...\n",
      "  페이지 4: 1010 글자 추출\n",
      "  첫 50글자: i\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "서울시, 다양한 교통약자 불편 개선 위\n",
      "페이지 5 처리 중...\n",
      "  페이지 5: 979 글자 추출\n",
      "  첫 50글자: ii\n",
      "요\n",
      "약\n",
      "서울 장시간 통근자 지원 시급…혼잡 완화·서비스 수준 보장 방안 필요\n",
      "교통 체\n",
      "페이지 6 처리 중...\n",
      "  페이지 6: 321 글자 추출\n",
      "  첫 50글자: iii\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "목차\n",
      "01 연구개요\n",
      "2\n",
      "1_연구배경\n",
      "페이지 7 처리 중...\n",
      "  페이지 7: 649 글자 추출\n",
      "  첫 50글자: iv\n",
      "목\n",
      "차\n",
      "표목차\n",
      "[표 2-1] 교통부문에서의 약자 정의와 권리, 책무 \n",
      "14\n",
      "[표 3-\n",
      "페이지 8 처리 중...\n",
      "  페이지 8: 752 글자 추출\n",
      "  첫 50글자: v\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "그림목차\n",
      "[그림 1-1] 약자의 범위 \n",
      "페이지 9 처리 중...\n",
      "  페이지 9: 1125 글자 추출\n",
      "  첫 50글자: vi\n",
      "목\n",
      "차\n",
      "[그림 3-15] 고령자가 걸어서 동네 외출 시 가장 불편한 점\n",
      "38\n",
      "[그림 \n",
      "페이지 10 처리 중...\n",
      "  페이지 10: 820 글자 추출\n",
      "  첫 50글자: vii\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "[그림 4-24] 현재 거주지 선택\n",
      "페이지 11 처리 중...\n",
      "  페이지 11: 32 글자 추출\n",
      "  첫 50글자: 01\n",
      "연구개요\n",
      "1_연구배경 및 목적\n",
      "2_연구내용 및 방법\n",
      "\n",
      "페이지 12 처리 중...\n",
      "  페이지 12: 1008 글자 추출\n",
      "  첫 50글자: 2\n",
      "01\n",
      "연\n",
      "구\n",
      "개\n",
      "요\n",
      "01. 연구개요\n",
      "1_연구배경 및 목적\n",
      "2005년 교통약자법이 제정된\n",
      "페이지 13 처리 중...\n",
      "  페이지 13: 771 글자 추출\n",
      "  첫 50글자: 3\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "에 부응하여, 교통약자 지원을 위한 교\n",
      "페이지 14 처리 중...\n",
      "  페이지 14: 605 글자 추출\n",
      "  첫 50글자: 4\n",
      "01\n",
      "연\n",
      "구\n",
      "개\n",
      "요\n",
      "정책 추진을 위한 재원과 서비스의 시장 규모를 검토하여 가능성을 확\n",
      "페이지 15 처리 중...\n",
      "  페이지 15: 51 글자 추출\n",
      "  첫 50글자: 02\n",
      "교통분야의 약자 범위 재설정 \n",
      "1_약자의 개념\n",
      "2_교통부문에서의 약자\n",
      "3_연구의 대상\n",
      "페이지 16 처리 중...\n",
      "  페이지 16: 1028 글자 추출\n",
      "  첫 50글자: 6\n",
      "02\n",
      "교\n",
      "통\n",
      "분\n",
      "야\n",
      "의\n",
      " 약\n",
      "자\n",
      " 범\n",
      "위\n",
      " 재\n",
      "설\n",
      "정\n",
      "02. 교통분야의 약자 범위 재설\n",
      "페이지 17 처리 중...\n",
      "  페이지 17: 1401 글자 추출\n",
      "  첫 50글자: 7\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "2) 약자의 조건4)\n",
      "약자는 상대적인 \n",
      "페이지 18 처리 중...\n",
      "  페이지 18: 1175 글자 추출\n",
      "  첫 50글자: 8\n",
      "02\n",
      "교\n",
      "통\n",
      "분\n",
      "야\n",
      "의\n",
      " 약\n",
      "자\n",
      " 범\n",
      "위\n",
      " 재\n",
      "설\n",
      "정\n",
      "(3) 권력의 열세(Differe\n",
      "페이지 19 처리 중...\n",
      "  페이지 19: 1227 글자 추출\n",
      "  첫 50글자: 9\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "시간적 조건과 사회적 환경에 따라 복합\n",
      "페이지 20 처리 중...\n",
      "  페이지 20: 1161 글자 추출\n",
      "  첫 50글자: 10\n",
      "02\n",
      "교\n",
      "통\n",
      "분\n",
      "야\n",
      "의\n",
      " 약\n",
      "자\n",
      " 범\n",
      "위\n",
      " 재\n",
      "설\n",
      "정\n",
      "(1) 신체적·정신적 약자\n",
      "신체\n",
      "페이지 21 처리 중...\n",
      "  페이지 21: 1209 글자 추출\n",
      "  첫 50글자: 11\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "(3) 경제적 약자\n",
      "경제적 약자는 소\n",
      "페이지 22 처리 중...\n",
      "  페이지 22: 627 글자 추출\n",
      "  첫 50글자: 12\n",
      "02\n",
      "교\n",
      "통\n",
      "분\n",
      "야\n",
      "의\n",
      " 약\n",
      "자\n",
      " 범\n",
      "위\n",
      " 재\n",
      "설\n",
      "정\n",
      "(5) 기타\n",
      "이외에도 우리 사회\n",
      "페이지 23 처리 중...\n",
      "  페이지 23: 1071 글자 추출\n",
      "  첫 50글자: 13\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "2_교통부문에서의 약자\n",
      "1) 기존의 \n",
      "페이지 24 처리 중...\n",
      "  페이지 24: 1525 글자 추출\n",
      "  첫 50글자: 14\n",
      "02\n",
      "교\n",
      "통\n",
      "분\n",
      "야\n",
      "의\n",
      " 약\n",
      "자\n",
      " 범\n",
      "위\n",
      " 재\n",
      "설\n",
      "정\n",
      "교통약자의 이동편의 증진법(약칭\n",
      "페이지 25 처리 중...\n",
      "  페이지 25: 1002 글자 추출\n",
      "  첫 50글자: 15\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "이와 같이, 기존의 교통약자뿐만 아니\n",
      "페이지 26 처리 중...\n",
      "  페이지 26: 425 글자 추출\n",
      "  첫 50글자: 16\n",
      "02\n",
      "교\n",
      "통\n",
      "분\n",
      "야\n",
      "의\n",
      " 약\n",
      "자\n",
      " 범\n",
      "위\n",
      " 재\n",
      "설\n",
      "정\n",
      "<서울동행맵 간편 가입>\n",
      "<서울\n",
      "페이지 27 처리 중...\n",
      "  페이지 27: 742 글자 추출\n",
      "  첫 50글자: 17\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "한편 경기도에서는 교통 선택권이 제한\n",
      "페이지 28 처리 중...\n",
      "  페이지 28: 995 글자 추출\n",
      "  첫 50글자: 18\n",
      "02\n",
      "교\n",
      "통\n",
      "분\n",
      "야\n",
      "의\n",
      " 약\n",
      "자\n",
      " 범\n",
      "위\n",
      " 재\n",
      "설\n",
      "정\n",
      "경제적 어려움을 겪는 시민들을 \n",
      "페이지 29 처리 중...\n",
      "  페이지 29: 940 글자 추출\n",
      "  첫 50글자: 19\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 서울특별시 보도자료(2022.\n",
      "페이지 30 처리 중...\n",
      "  페이지 30: 112 글자 추출\n",
      "  첫 50글자: 20\n",
      "02\n",
      "교\n",
      "통\n",
      "분\n",
      "야\n",
      "의\n",
      " 약\n",
      "자\n",
      " 범\n",
      "위\n",
      " 재\n",
      "설\n",
      "정\n",
      "자료: 서울특별시 누리집, ht\n",
      "페이지 31 처리 중...\n",
      "  페이지 31: 1223 글자 추출\n",
      "  첫 50글자: 21\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "3_연구의 대상\n",
      "이 연구에서는 다양한\n",
      "페이지 32 처리 중...\n",
      "  페이지 32: 195 글자 추출\n",
      "  첫 50글자: 22\n",
      "02\n",
      "교\n",
      "통\n",
      "분\n",
      "야\n",
      "의\n",
      " 약\n",
      "자\n",
      " 범\n",
      "위\n",
      " 재\n",
      "설\n",
      "정\n",
      "이와 같이 고령자, 장시간 통근\n",
      "페이지 33 처리 중...\n",
      "  페이지 33: 41 글자 추출\n",
      "  첫 50글자: 03\n",
      "고령자\n",
      "1_고령자 지원 정책 진단\n",
      "2_고령자를 위한 지원 정책 제언\n",
      "\n",
      "페이지 34 처리 중...\n",
      "  페이지 34: 494 글자 추출\n",
      "  첫 50글자: 24\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "03. 고령자\n",
      "1_고령자 지원 정책 진단\n",
      "1) 고령자 통행 특성 및 \n",
      "페이지 35 처리 중...\n",
      "  페이지 35: 713 글자 추출\n",
      "  첫 50글자: 25\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "고령자의 이동 능력과 사회적 특성을 \n",
      "페이지 36 처리 중...\n",
      "  페이지 36: 630 글자 추출\n",
      "  첫 50글자: 26\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "한편 신체 능력이 저하되었음에도 불구하고 여전히 승용차를 이용하고자 \n",
      "페이지 37 처리 중...\n",
      "  페이지 37: 705 글자 추출\n",
      "  첫 50글자: 27\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 서울특별시(2024), 202\n",
      "페이지 38 처리 중...\n",
      "  페이지 38: 814 글자 추출\n",
      "  첫 50글자: 28\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "자료: 한국도로교통공단(2024), 2024년판(2023년 통계) 교\n",
      "페이지 39 처리 중...\n",
      "  페이지 39: 850 글자 추출\n",
      "  첫 50글자: 29\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "하고 있다. 다만 우리나라와는 달리 \n",
      "페이지 40 처리 중...\n",
      "  페이지 40: 1263 글자 추출\n",
      "  첫 50글자: 30\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "이러한 면허 유지를 위한 규제와 함께, 고령 운전자의 안전을 지원하기\n",
      "페이지 41 처리 중...\n",
      "  페이지 41: 716 글자 추출\n",
      "  첫 50글자: 31\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "더라도 자립적인 생활과 이동성을 유지\n",
      "페이지 42 처리 중...\n",
      "  페이지 42: 792 글자 추출\n",
      "  첫 50글자: 32\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "그러나 우리나라의 고령 운전자를 대상으로 한 운전면허 갱신 조건은 다\n",
      "페이지 43 처리 중...\n",
      "  페이지 43: 527 글자 추출\n",
      "  첫 50글자: 33\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 매일건설신문(2020. 4. \n",
      "페이지 44 처리 중...\n",
      "  페이지 44: 631 글자 추출\n",
      "  첫 50글자: 34\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "셋째, 고령자들의 편리한 대중교통 이용을 위해 “1역사 1동선” 정책\n",
      "페이지 45 처리 중...\n",
      "  페이지 45: 813 글자 추출\n",
      "  첫 50글자: 35\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "또한 서울시는 교통약자의 보행권을 보\n",
      "페이지 46 처리 중...\n",
      "  페이지 46: 1055 글자 추출\n",
      "  첫 50글자: 36\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "2_고령자를 위한 지원 정책 제언\n",
      "1) 공공분야에서의 정책적 지원\n",
      "(\n",
      "페이지 47 처리 중...\n",
      "  페이지 47: 636 글자 추출\n",
      "  첫 50글자: 37\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 국민건강보험(2014), 장애\n",
      "페이지 48 처리 중...\n",
      "  페이지 48: 733 글자 추출\n",
      "  첫 50글자: 38\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "절한 간격으로 벤치와 쉼터 같은 휴게시설을 설치하는 것이 필요하다. \n",
      "페이지 49 처리 중...\n",
      "  페이지 49: 661 글자 추출\n",
      "  첫 50글자: 39\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 경향신문(2021. 4. 18\n",
      "페이지 50 처리 중...\n",
      "  페이지 50: 692 글자 추출\n",
      "  첫 50글자: 40\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "우선순위 설정지표\n",
      "에스컬레이터\n",
      "엘리베이터\n",
      "이용자수요\n",
      "전체 이용자 수\n",
      "\n",
      "페이지 51 처리 중...\n",
      "  페이지 51: 719 글자 추출\n",
      "  첫 50글자: 41\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "서울시 지하철 출입구의 에스컬레이터 \n",
      "페이지 52 처리 중...\n",
      "  페이지 52: 849 글자 추출\n",
      "  첫 50글자: 42\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "[그림 3-19] 지하철 출입구의 에스컬레이터 설치 현황\n",
      "에스컬레이터\n",
      "페이지 53 처리 중...\n",
      "  페이지 53: 1070 글자 추출\n",
      "  첫 50글자: 43\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "으로써, 공간과 사람에 대한 접근을 \n",
      "페이지 54 처리 중...\n",
      "  페이지 54: 1128 글자 추출\n",
      "  첫 50글자: 44\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "특히 대부분의 실행 과제가 일반회계와 도시개발특별회계로 편성되어 추진\n",
      "페이지 55 처리 중...\n",
      "  페이지 55: 468 글자 추출\n",
      "  첫 50글자: 45\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "서 밀리는 고령인구, 교통약자와 취약\n",
      "페이지 56 처리 중...\n",
      "  페이지 56: 1087 글자 추출\n",
      "  첫 50글자: 46\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "2) 민간분야에서의 고령자 이동지원 서비스 제공 방안\n",
      "(1) 민간분야\n",
      "페이지 57 처리 중...\n",
      "  페이지 57: 915 글자 추출\n",
      "  첫 50글자: 47\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 1) MTA, 2019, ht\n",
      "페이지 58 처리 중...\n",
      "  페이지 58: 867 글자 추출\n",
      "  첫 50글자: 48\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "(2) 민간 서비스 운영을 위한 시장성 검토\n",
      "민간 서비스가 성공적으로\n",
      "페이지 59 처리 중...\n",
      "  페이지 59: 882 글자 추출\n",
      "  첫 50글자: 49\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "한편 설문조사를 통해 승용차의 소유 \n",
      "페이지 60 처리 중...\n",
      "  페이지 60: 826 글자 추출\n",
      "  첫 50글자: 50\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "구분\n",
      "항목\n",
      "직접 사적 비용\n",
      "소유\n",
      "구입 관련 순수 발생 비용\n",
      "보험료\n",
      "거\n",
      "페이지 61 처리 중...\n",
      "  페이지 61: 1112 글자 추출\n",
      "  첫 50글자: 51\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "②보험료\n",
      "자동차를 소유한 사람은 의무\n",
      "페이지 62 처리 중...\n",
      "  페이지 62: 1106 글자 추출\n",
      "  첫 50글자: 52\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "순수 유류가격은 2023년 월평균 주유소와 대리점의 세전금액의 연평균\n",
      "페이지 63 처리 중...\n",
      "  페이지 63: 1007 글자 추출\n",
      "  첫 50글자: 53\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "9백만 대가량이다. \n",
      "이를 활용하여 \n",
      "페이지 64 처리 중...\n",
      "  페이지 64: 1092 글자 추출\n",
      "  첫 50글자: 54\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "구분\n",
      "세법\n",
      "내용\n",
      "자동차세\n",
      "지방세법 제127조\n",
      "(과세표준과 세율)\n",
      "∙ \n",
      "페이지 65 처리 중...\n",
      "  페이지 65: 1364 글자 추출\n",
      "  첫 50글자: 55\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "조사되었다. 이번 연구의 분석 결과와\n",
      "페이지 66 처리 중...\n",
      "  페이지 66: 430 글자 추출\n",
      "  첫 50글자: 56\n",
      "03\n",
      "고\n",
      "령\n",
      "자\n",
      "이를 쉽게 비교 가능한 주요 산업의 연간 매출 규모와 대조해 보면, \n",
      "페이지 67 처리 중...\n",
      "  페이지 67: 50 글자 추출\n",
      "  첫 50글자: 04\n",
      "장시간 통행자\n",
      "1_장시간 통행자 현황과 문제점\n",
      "2_대중교통서비스 보상제도 도입 제안\n",
      "\n",
      "페이지 68 처리 중...\n",
      "  페이지 68: 795 글자 추출\n",
      "  첫 50글자: 58\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "04. 장시간 통행자\n",
      "1_장시간 통행자 현황과 문제점\n",
      "1\n",
      "페이지 69 처리 중...\n",
      "  페이지 69: 372 글자 추출\n",
      "  첫 50글자: 59\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 통계청(각 연도), 「인구총조\n",
      "페이지 70 처리 중...\n",
      "  페이지 70: 1154 글자 추출\n",
      "  첫 50글자: 60\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "2) 서울시민의 장시간 통근·통학 현황\n",
      "서울시의 2021\n",
      "페이지 71 처리 중...\n",
      "  페이지 71: 688 글자 추출\n",
      "  첫 50글자: 61\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "최근 서울의 통근·통학 시간을 조사한\n",
      "페이지 72 처리 중...\n",
      "  페이지 72: 895 글자 추출\n",
      "  첫 50글자: 62\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "반면 상당수의 서울 시민은 장시간 통근·통학도 경험하고 \n",
      "페이지 73 처리 중...\n",
      "  페이지 73: 926 글자 추출\n",
      "  첫 50글자: 63\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "소득 수준에 따라 통근시간 1분당 경\n",
      "페이지 74 처리 중...\n",
      "  페이지 74: 563 글자 추출\n",
      "  첫 50글자: 64\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "삶의 질 악화에 기여하는 부정적 방향 지표(-)로 활용하\n",
      "페이지 75 처리 중...\n",
      "  페이지 75: 399 글자 추출\n",
      "  첫 50글자: 65\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      " \n",
      "자료: 서울시(2023), 서울 \n",
      "페이지 76 처리 중...\n",
      "  페이지 76: 305 글자 추출\n",
      "  첫 50글자: 66\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "자료: 서울특별시(2023), 서울서베이 가구 및 가구원\n",
      "페이지 77 처리 중...\n",
      "  페이지 77: 658 글자 추출\n",
      "  첫 50글자: 67\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 서울특별시(2023), 서울서\n",
      "페이지 78 처리 중...\n",
      "  페이지 78: 598 글자 추출\n",
      "  첫 50글자: 68\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "4) 장시간 통행자와 교통약자\n",
      "이 장에서는 새로운 교통약\n",
      "페이지 79 처리 중...\n",
      "  페이지 79: 878 글자 추출\n",
      "  첫 50글자: 69\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "한편 장시간 출퇴근이 개인 삶의 만족\n",
      "페이지 80 처리 중...\n",
      "  페이지 80: 906 글자 추출\n",
      "  첫 50글자: 70\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "5) 장시간 통행자 특성 분석 및 시사점 도출\n",
      "(1) 설\n",
      "페이지 81 처리 중...\n",
      "  페이지 81: 740 글자 추출\n",
      "  첫 50글자: 71\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "구분\n",
      "설문 항목\n",
      "기본 속성\n",
      "- 성별 \n",
      "페이지 82 처리 중...\n",
      "  페이지 82: 600 글자 추출\n",
      "  첫 50글자: 72\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "(2) 설문조사 결과\n",
      "응답 결과 편도 1시간 미만 통행 \n",
      "페이지 83 처리 중...\n",
      "  페이지 83: 412 글자 추출\n",
      "  첫 50글자: 73\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "적으로 낮고, 인천/경기도가 목적지인\n",
      "페이지 84 처리 중...\n",
      "  페이지 84: 446 글자 추출\n",
      "  첫 50글자: 74\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "출퇴근 및 등하교 시 주로 이용하는 교통수단으로는 총 통\n",
      "페이지 85 처리 중...\n",
      "  페이지 85: 668 글자 추출\n",
      "  첫 50글자: 75\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "이외에도 사회 활동 만족도와 업무 효\n",
      "페이지 86 처리 중...\n",
      "  페이지 86: 442 글자 추출\n",
      "  첫 50글자: 76\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      " \n",
      "[그림 4-23] 거주지 선택에서 통근시간의 중요도(\n",
      "페이지 87 처리 중...\n",
      "  페이지 87: 148 글자 추출\n",
      "  첫 50글자: 77\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "중교통 이용자는 ‘차내 혼잡>(통행시\n",
      "페이지 88 처리 중...\n",
      "  페이지 88: 817 글자 추출\n",
      "  첫 50글자: 78\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "2_대중교통서비스 보상제도 도입 제안\n",
      "1) 중앙정부의 교\n",
      "페이지 89 처리 중...\n",
      "  페이지 89: 1077 글자 추출\n",
      "  첫 50글자: 79\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "2) 서울시 약자 동행 프로그램 제안\n",
      "페이지 90 처리 중...\n",
      "  페이지 90: 1113 글자 추출\n",
      "  첫 50글자: 80\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "이상 60분 미만은 25%, 60분 이상은 50% 수준이\n",
      "페이지 91 처리 중...\n",
      "  페이지 91: 666 글자 추출\n",
      "  첫 50글자: 81\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 내 손안에 서울(2024. 4\n",
      "페이지 92 처리 중...\n",
      "  페이지 92: 679 글자 추출\n",
      "  첫 50글자: 82\n",
      "04\n",
      "장\n",
      "시\n",
      "간\n",
      " 통\n",
      "행\n",
      "자\n",
      "의 매력도가 증가할 것으로 예상된다. 이를 통해 승용차 \n",
      "페이지 93 처리 중...\n",
      "  페이지 93: 48 글자 추출\n",
      "  첫 50글자: 05\n",
      "심야 통행자\n",
      "1_사회적 약자로서의 심야 통행자\n",
      "2_심야 통행자를 위한 지원 방안\n",
      "\n",
      "페이지 94 처리 중...\n",
      "  페이지 94: 377 글자 추출\n",
      "  첫 50글자: 84\n",
      "05\n",
      "심\n",
      "야\n",
      " 통\n",
      "행\n",
      "자\n",
      "05. 심야 통행자\n",
      "1_사회적 약자로서의 심야 통행자\n",
      "1) \n",
      "페이지 95 처리 중...\n",
      "  페이지 95: 1096 글자 추출\n",
      "  첫 50글자: 85\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "여전히 높은 수준을 유지하고 있으며,\n",
      "페이지 96 처리 중...\n",
      "  페이지 96: 551 글자 추출\n",
      "  첫 50글자: 86\n",
      "05\n",
      "심\n",
      "야\n",
      " 통\n",
      "행\n",
      "자\n",
      "또한 ‘퇴근 후 너무 피곤하여 집안일을 할 수 없다’는 응답\n",
      "페이지 97 처리 중...\n",
      "  페이지 97: 1137 글자 추출\n",
      "  첫 50글자: 87\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 한국산업환경보건공단(2023)\n",
      "페이지 98 처리 중...\n",
      "  페이지 98: 847 글자 추출\n",
      "  첫 50글자: 88\n",
      "05\n",
      "심\n",
      "야\n",
      " 통\n",
      "행\n",
      "자\n",
      "자료: 카카오모빌리티(2022), 빅데이터로 살펴본 택시 대\n",
      "페이지 99 처리 중...\n",
      "  페이지 99: 740 글자 추출\n",
      "  첫 50글자: 89\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "자료: 스마트카드 및 SKT 통신자료\n",
      "페이지 100 처리 중...\n",
      "  페이지 100: 554 글자 추출\n",
      "  첫 50글자: 90\n",
      "05\n",
      "심\n",
      "야\n",
      " 통\n",
      "행\n",
      "자\n",
      "[그림 5-8] 심야 통행 이유(좌) 및 근무지 유지 이유(\n",
      "페이지 101 처리 중...\n",
      "  페이지 101: 980 글자 추출\n",
      "  첫 50글자: 91\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "심야 통행으로 느끼는 스트레스에 대해\n",
      "페이지 102 처리 중...\n",
      "  페이지 102: 574 글자 추출\n",
      "  첫 50글자: 92\n",
      "05\n",
      "심\n",
      "야\n",
      " 통\n",
      "행\n",
      "자\n",
      "해당 설문 결과를 통해, 심야 통행이 해결해야 할 가장 큰 \n",
      "페이지 103 처리 중...\n",
      "  페이지 103: 1220 글자 추출\n",
      "  첫 50글자: 93\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "2_심야 통행자를 위한 지원 방안\n",
      "앞\n",
      "페이지 104 처리 중...\n",
      "  페이지 104: 812 글자 추출\n",
      "  첫 50글자: 94\n",
      "05\n",
      "심\n",
      "야\n",
      " 통\n",
      "행\n",
      "자\n",
      "[그림 5-12] 심야시간대의 선호 교통수단 및 이유\n",
      "심야 \n",
      "페이지 105 처리 중...\n",
      "  페이지 105: 851 글자 추출\n",
      "  첫 50글자: 95\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "[그림 5-13] 대안수단별 지불의사\n",
      "페이지 106 처리 중...\n",
      "  페이지 106: 1252 글자 추출\n",
      "  첫 50글자: 96\n",
      "05\n",
      "심\n",
      "야\n",
      " 통\n",
      "행\n",
      "자\n",
      "영하는 것이다. 심야 통행이 많은 지역을 중심으로 DRT 서\n",
      "페이지 107 처리 중...\n",
      "  페이지 107: 534 글자 추출\n",
      "  첫 50글자: 97\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "[그림 5-14] 심야 통행 취약지역\n",
      "페이지 108 처리 중...\n",
      "  페이지 108: 899 글자 추출\n",
      "  첫 50글자: 98\n",
      "05\n",
      "심\n",
      "야\n",
      " 통\n",
      "행\n",
      "자\n",
      "보다 구체적으로 야간통행의 비율이 높은 강북구와 중랑구의 심\n",
      "페이지 109 처리 중...\n",
      "  페이지 109: 1111 글자 추출\n",
      "  첫 50글자: 99\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "존재하나 공급이 제한된 시간대에 효과\n",
      "페이지 110 처리 중...\n",
      "  페이지 110: 1261 글자 추출\n",
      "  첫 50글자: 100\n",
      "05\n",
      "심\n",
      "야\n",
      " 통\n",
      "행\n",
      "자\n",
      "여객자동차 운수사업법 시행령\n",
      "제2조의2(수요응답형 여객자동\n",
      "페이지 111 처리 중...\n",
      "  페이지 111: 970 글자 추출\n",
      "  첫 50글자: 101\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "안전하고 편리한 이동을 위해 대중교\n",
      "페이지 112 처리 중...\n",
      "  페이지 112: 텍스트 없음\n",
      "페이지 113 처리 중...\n",
      "  페이지 113: 1201 글자 추출\n",
      "  첫 50글자: 103\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "참고문헌\n",
      "대도시권광역교통위원회, 2\n",
      "페이지 114 처리 중...\n",
      "  페이지 114: 31 글자 추출\n",
      "  첫 50글자: 104\n",
      "참\n",
      "고\n",
      "문\n",
      "헌\n",
      "부록 1_장거리 통행자 설문조사지\n",
      "\n",
      "페이지 115 처리 중...\n",
      "  페이지 115: 31 글자 추출\n",
      "  첫 50글자: 105\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "\n",
      "페이지 116 처리 중...\n",
      "  페이지 116: 12 글자 추출\n",
      "  첫 50글자: 106\n",
      "참\n",
      "고\n",
      "문\n",
      "헌\n",
      "\n",
      "페이지 117 처리 중...\n",
      "  페이지 117: 31 글자 추출\n",
      "  첫 50글자: 107\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "\n",
      "페이지 118 처리 중...\n",
      "  페이지 118: 12 글자 추출\n",
      "  첫 50글자: 108\n",
      "참\n",
      "고\n",
      "문\n",
      "헌\n",
      "\n",
      "페이지 119 처리 중...\n",
      "  페이지 119: 31 글자 추출\n",
      "  첫 50글자: 109\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "\n",
      "페이지 120 처리 중...\n",
      "  페이지 120: 32 글자 추출\n",
      "  첫 50글자: 110\n",
      "참\n",
      "고\n",
      "문\n",
      "헌\n",
      "부록 2_심야시간 통행자 설문조사지\n",
      "\n",
      "페이지 121 처리 중...\n",
      "  페이지 121: 31 글자 추출\n",
      "  첫 50글자: 111\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "\n",
      "페이지 122 처리 중...\n",
      "  페이지 122: 12 글자 추출\n",
      "  첫 50글자: 112\n",
      "참\n",
      "고\n",
      "문\n",
      "헌\n",
      "\n",
      "페이지 123 처리 중...\n",
      "  페이지 123: 31 글자 추출\n",
      "  첫 50글자: 113\n",
      "약\n",
      "자\n",
      "와\n",
      " 동\n",
      "행\n",
      "하\n",
      "는\n",
      " 서\n",
      "울\n",
      "의\n",
      " 교\n",
      "통\n",
      "\n",
      "페이지 124 처리 중...\n",
      "  페이지 124: 1557 글자 추출\n",
      "  첫 50글자: 114\n",
      "Abstract\n",
      "Abstract\n",
      "Seoul's Inclusive Transporta\n",
      "페이지 125 처리 중...\n",
      "  페이지 125: 2202 글자 추출\n",
      "  첫 50글자: 115\n",
      "Seoul's Inclusive Transportation for the Disad\n",
      "페이지 126 처리 중...\n",
      "  페이지 126: 697 글자 추출\n",
      "  첫 50글자: 116\n",
      "Contents\n",
      "Contents\n",
      "01 Research Overview\n",
      "1_Resea\n",
      "페이지 127 처리 중...\n",
      "  페이지 127: 157 글자 추출\n",
      "  첫 50글자: 약자와 동행하는 서울의 교통\n",
      "서울연 2024-PR-15\n",
      "발행인  오균\n",
      "발행일  2025년 \n",
      "\n",
      "총 추출된 텍스트: 91332 글자\n",
      "✓ 텍스트 추출 성공!\n",
      "전체 텍스트 첫 200글자:\n",
      "약자와 동행하는 서울의 교통\n",
      "이신해   김승준   한영준   양재환 \n",
      "윤서연   연준형   김영범   김지한\n",
      "\n",
      "\n",
      "약자와 동행하는 서울의 교통\n",
      "\n",
      "\n",
      "이 보고서의 내용은 연구진의 견해로서 \n",
      "서울특별시의 정책과는 다를 수도 있습니다.\n",
      "연구책임\n",
      "이신해 서울연구원 스마트교통연구실 선임연구위원\n",
      "연구진\n",
      "김승준 서울연구원 스마트교통연구실 선임연구위원\n",
      "한영준 서울연구원 스마트교통연구실 연구위원\n",
      "양재환 서울연구원 스마트교통연구실 연구위원\n",
      "윤서연 서울연구원 미래공간연구실 연구위원\n",
      "연준형 서울연구원 스마트교통연구실 연구원\n",
      "김영범 서울연구원 스마트교통연구실\n"
     ]
    }
   ],
   "source": [
    "import fitz  # pymupdf\n",
    "import os\n",
    "\n",
    "def extract_text_with_fitz(pdf_path):\n",
    "    \"\"\"PyMuPDF로 텍스트 추출 - 한글 처리 개선\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        print(f\"PDF 파일: {pdf_path}\")\n",
    "        print(f\"총 페이지 수: {len(doc)}\")\n",
    "        \n",
    "        full_text = \"\"\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            print(f\"페이지 {page_num + 1} 처리 중...\")\n",
    "            page = doc[page_num]\n",
    "            \n",
    "            # 텍스트 추출\n",
    "            text = page.get_text()\n",
    "            \n",
    "            if text and text.strip():\n",
    "                # 한글 처리 및 텍스트 정리\n",
    "                text = text.replace('\\x00', '')  # null 문자 제거\n",
    "                text = text.replace('\\ufeff', '')  # BOM 제거\n",
    "                text = text.replace('\\r\\n', '\\n')  # 줄바꿈 정리\n",
    "                text = text.replace('\\r', '\\n')\n",
    "                \n",
    "                print(f\"  페이지 {page_num + 1}: {len(text)} 글자 추출\")\n",
    "                print(f\"  첫 50글자: {text[:50]}\")\n",
    "                \n",
    "                full_text += text + \"\\n\\n\"  # 페이지 구분\n",
    "            else:\n",
    "                print(f\"  페이지 {page_num + 1}: 텍스트 없음\")\n",
    "        \n",
    "        doc.close()\n",
    "        print(f\"\\n총 추출된 텍스트: {len(full_text)} 글자\")\n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PyMuPDF 오류: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 텍스트 추출\n",
    "pdf_file = \"2024-PR-15.pdf\"\n",
    "full_text = extract_text_with_fitz(pdf_file)\n",
    "\n",
    "if full_text:\n",
    "    print(\"✓ 텍스트 추출 성공!\")\n",
    "    print(f\"전체 텍스트 첫 200글자:\")\n",
    "    print(full_text[:300])\n",
    "else:\n",
    "    print(\"✗ 텍스트 추출 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35f2f3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 청킹 결과 ===\n",
      "총 청크 개수: 49\n",
      "청크 길이 - 최소: 129, 최대: 2000, 평균: 1961.4\n",
      "\n",
      "첫 번째 청크 미리보기:\n",
      "약자와 동행하는 서울의 교통\n",
      "이신해   김승준   한영준   양재환 \n",
      "윤서연   연준형   김영범   김지한\n",
      "\n",
      "\n",
      "약자와 동행하는 서울의 교통\n",
      "\n",
      "\n",
      "이 보고서의 내용은 연구진의 견해로서 \n",
      "서울특별시의 정책과는 다를 수도 있습니다.\n",
      "연구책임\n",
      "이신해 서울연구원 스마트교통연구실 선임연구위원\n",
      "연구진\n",
      "김승준 서울연구원 스마트교통연구실 선임연구위원\n",
      "한영준 서울연구원 스마...\n",
      "\n",
      "마지막 청크 미리보기:\n",
      "-15\n",
      "발행인  오균\n",
      "발행일  2025년 5월 30일\n",
      "발행처  서울연구원\n",
      "ISBN  979-11-5700-908-4 95350  비매품\n",
      "06756 서울특별시 서초구 남부순환로 340길 57\n",
      "이 출판물의 판권은 서울연구원에 속합니다.\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=2000, overlap=100):\n",
    "    \"\"\"텍스트를 청크로 나누는 함수\"\"\"\n",
    "    if not text.strip():\n",
    "        print(\"빈 텍스트입니다.\")\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end].strip()\n",
    "        \n",
    "        if chunk:  # 빈 청크가 아닌 경우만 추가\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# 텍스트 청킹\n",
    "if full_text:\n",
    "    chunks = chunk_text(full_text)\n",
    "    \n",
    "    print(f\"\\n=== 청킹 결과 ===\")\n",
    "    print(f\"총 청크 개수: {len(chunks)}\")\n",
    "    \n",
    "    if chunks:\n",
    "        chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "        print(f\"청크 길이 - 최소: {min(chunk_lengths)}, 최대: {max(chunk_lengths)}, 평균: {sum(chunk_lengths)/len(chunk_lengths):.1f}\")\n",
    "        \n",
    "        print(f\"\\n첫 번째 청크 미리보기:\")\n",
    "        print(chunks[0][:200] + \"...\" if len(chunks[0]) > 200 else chunks[0])\n",
    "        \n",
    "        print(f\"\\n마지막 청크 미리보기:\")\n",
    "        print(chunks[-1][:200] + \"...\" if len(chunks[-1]) > 200 else chunks[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7876fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 벡터 스토어 처리 ===\n",
      "기존 벡터 스토어를 찾았습니다. 로드 중...\n",
      "✓ 동일한 PDF 파일의 벡터 스토어 로드 성공!\n",
      "- 벡터 개수: 49\n",
      "- 청크 개수: 49\n",
      "- 원본 파일: 2024-PR-15.pdf\n",
      "- 임베딩 모델: text-embedding-3-large\n",
      "✓ 벡터 스토어 준비 완료!\n",
      "이제 검색을 할 수 있습니다!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# OpenAI API 키 설정 (환경변수에서 읽어오거나 직접 설정)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY  # 필요한 경우\n",
    "\n",
    "def save_vectorstore_openai(index, chunks, metadatas, pdf_filename, save_name=\"pdf_openai_vectors\"):\n",
    "    \"\"\"벡터 스토어 저장\"\"\"\n",
    "    try:\n",
    "        # FAISS 인덱스 저장\n",
    "        faiss.write_index(index, f'{save_name}_vectors.index')\n",
    "        \n",
    "        # 청크와 메타데이터 저장\n",
    "        with open(f'{save_name}_data.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'chunks': chunks,\n",
    "                'metadatas': metadatas,\n",
    "                'pdf_filename': pdf_filename,\n",
    "                'chunk_count': len(chunks),\n",
    "                'embedding_model': 'text-embedding-3-large',\n",
    "                'embedding_type': 'openai'\n",
    "            }, f)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"저장 오류: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_faiss_vectorstore_openai(chunks, pdf_filename, save_name=\"pdf_openai_vectors\"):\n",
    "    \"\"\"청크들을 FAISS 벡터 스토어로 변환하고 자동 저장 (OpenAI 임베딩 사용)\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"청크가 없습니다.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    print(\"OpenAI 임베딩 모델 로드 중...\")\n",
    "    embedding = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "    \n",
    "    print(f\"임베딩 생성 중... ({len(chunks)}개 청크)\")\n",
    "    print(\"OpenAI API 호출 중이므로 시간이 걸릴 수 있습니다...\")\n",
    "    \n",
    "    try:\n",
    "        # 청크들을 임베딩으로 변환\n",
    "        embeddings = embedding.embed_documents(chunks)\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        print(f\"임베딩 완료! 차원: {embeddings.shape[1]}, 개수: {embeddings.shape[0]}\")\n",
    "        \n",
    "        # FAISS 인덱스 생성\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)  # L2 거리 기반 인덱스\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        # 메타데이터 생성\n",
    "        metadatas = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            metadatas.append({\n",
    "                'chunk_id': i,\n",
    "                'source': pdf_filename,\n",
    "                'chunk_size': len(chunk),\n",
    "                'preview': chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
    "            })\n",
    "        \n",
    "        print(f\"FAISS 벡터 스토어 생성 완료! {index.ntotal}개 벡터 저장됨\")\n",
    "        \n",
    "        # 자동 저장\n",
    "        print(\"벡터 스토어를 자동 저장 중...\")\n",
    "        if save_vectorstore_openai(index, chunks, metadatas, pdf_filename, save_name):\n",
    "            print(\"✓ 자동 저장 완료!\")\n",
    "        else:\n",
    "            print(\"⚠️  자동 저장 실패\")\n",
    "        \n",
    "        return index, embedding, chunks, metadatas\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"임베딩 생성 오류: {e}\")\n",
    "        print(\"OpenAI API 키가 설정되어 있는지 확인해주세요.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def load_or_create_vectorstore(chunks, pdf_filename, save_name=\"pdf_openai_vectors\"):\n",
    "    \"\"\"\n",
    "    벡터 스토어가 있으면 로드하고, 없으면 새로 생성하는 함수\n",
    "    \"\"\"\n",
    "    index_file = f'{save_name}_vectors.index'\n",
    "    data_file = f'{save_name}_data.pkl'\n",
    "    \n",
    "    # 저장된 파일들이 모두 존재하는지 확인\n",
    "    if os.path.exists(index_file) and os.path.exists(data_file):\n",
    "        print(\"기존 벡터 스토어를 찾았습니다. 로드 중...\")\n",
    "        \n",
    "        try:\n",
    "            # 저장된 데이터 로드\n",
    "            index = faiss.read_index(index_file)\n",
    "            \n",
    "            with open(data_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            # 저장된 PDF 파일명과 현재 파일명 비교\n",
    "            if data['pdf_filename'] == pdf_filename:\n",
    "                print(\"✓ 동일한 PDF 파일의 벡터 스토어 로드 성공!\")\n",
    "                \n",
    "                # OpenAI 임베딩 모델 로드\n",
    "                embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "                \n",
    "                print(f\"- 벡터 개수: {index.ntotal}\")\n",
    "                print(f\"- 청크 개수: {data['chunk_count']}\")\n",
    "                print(f\"- 원본 파일: {data['pdf_filename']}\")\n",
    "                print(f\"- 임베딩 모델: {data.get('embedding_model', '정보 없음')}\")\n",
    "                \n",
    "                return index, embedding_model, data['chunks'], data['metadatas']\n",
    "            else:\n",
    "                print(f\"⚠️  다른 PDF 파일의 벡터 스토어입니다.\")\n",
    "                print(f\"   저장된 파일: {data['pdf_filename']}\")\n",
    "                print(f\"   현재 파일: {pdf_filename}\")\n",
    "                print(\"   새로운 벡터 스토어를 생성합니다...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  벡터 스토어 로드 중 오류 발생: {e}\")\n",
    "            print(\"   새로운 벡터 스토어를 생성합니다...\")\n",
    "    \n",
    "    else:\n",
    "        print(\"저장된 벡터 스토어를 찾을 수 없습니다. 새로 생성합니다...\")\n",
    "    \n",
    "    # 새로운 벡터 스토어 생성\n",
    "    print(\"\\n=== 새로운 벡터 스토어 생성 ===\")\n",
    "    return create_faiss_vectorstore_openai(chunks, pdf_filename, save_name)\n",
    "\n",
    "# FAISS 벡터 스토어 자동 로드/생성\n",
    "if 'chunks' in locals() and chunks:\n",
    "    print(\"=== 벡터 스토어 처리 ===\")\n",
    "    index, embedding_model, chunks, metadatas = load_or_create_vectorstore(\n",
    "        chunks, \n",
    "        pdf_file, \n",
    "        save_name=\"my_pdf_vectors\"  # 원하는 저장 이름으로 변경 가능\n",
    "    )\n",
    "    \n",
    "    if index is not None:\n",
    "        print(\"✓ 벡터 스토어 준비 완료!\")\n",
    "        print(\"이제 검색을 할 수 있습니다!\")\n",
    "    else:\n",
    "        print(\"✗ 벡터 스토어 준비 실패\")\n",
    "else:\n",
    "    print(\"청크가 없습니다. 먼저 텍스트 추출과 청킹을 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a713344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade langgraph>=0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1220d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 필요한 라이브러리들\n",
    "# from langgraph.graph import StateGraph, END\n",
    "# from langgraph.prebuilt import Toolnode\n",
    "# from langchain_core.tools import tool\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "# from typing import TypedDict, List, Dict, Any\n",
    "# import json\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode  # ✅ ToolExecutor 대신 ToolNode 사용\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f12d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langgraph langchain-core langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3e48647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: List[Any]\n",
    "    user_query: str\n",
    "    search_results: List[Dict]\n",
    "    summary: str\n",
    "    tool_calls: List[Dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7880d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_session_state():\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    if 'file_processed' not in st.session_state:\n",
    "        st.session_state.file_processed = False\n",
    "    if 'chunks' not in st.session_state:\n",
    "        st.session_state.chunks = []\n",
    "    if 'index' not in st.session_state:\n",
    "        st.session_state.index = None\n",
    "    if 'embedding_model' not in st.session_state:\n",
    "        st.session_state.embedding_model = None\n",
    "    if 'metadatas' not in st.session_state:\n",
    "        st.session_state.metadatas = []\n",
    "    if 'agent_graph' not in st.session_state:\n",
    "        st.session_state.agent_graph = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1b0e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_fitz(pdf_path):\n",
    "    \"\"\"PyMuPDF로 텍스트 추출\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\"\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            text = page.get_text()\n",
    "            \n",
    "            if text and text.strip():\n",
    "                text = text.replace('\\x00', '')\n",
    "                text = text.replace('\\ufeff', '')\n",
    "                text = text.replace('\\r\\n', '\\n')\n",
    "                text = text.replace('\\r', '\\n')\n",
    "                full_text += text + \"\\n\\n\"\n",
    "        \n",
    "        doc.close()\n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"PDF 텍스트 추출 오류: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, chunk_size=2000, overlap=100):\n",
    "    \"\"\"텍스트를 청크로 나누는 함수\"\"\"\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end].strip()\n",
    "        \n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "859b4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 셀 추가 (cell [28])\n",
    "import numpy as np\n",
    "\n",
    "def search_documents_openai(query, index, embedding_model, chunks, metadatas, k=3):\n",
    "    \"\"\"\n",
    "    OpenAI 임베딩을 사용한 문서 검색\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 쿼리를 임베딩으로 변환\n",
    "        query_embedding = embedding_model.embed_query(query)\n",
    "        query_embedding = np.array([query_embedding]).astype('float32')\n",
    "        \n",
    "        # FAISS로 유사한 문서 검색\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        \n",
    "        # 결과 포맷팅\n",
    "        results = []\n",
    "        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            if idx < len(chunks):  # 유효한 인덱스인지 확인\n",
    "                results.append({\n",
    "                    'chunk_id': idx,\n",
    "                    'content': chunks[idx],\n",
    "                    'score': 1.0 - (distance / 2.0),  # 거리를 유사도로 변환\n",
    "                    'metadata': metadatas[idx] if idx < len(metadatas) else {}\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"검색 오류: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e617837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 벡터 스토어 처리 ===\n",
      "기존 벡터 스토어를 찾았습니다. 로드 중...\n",
      "✓ 동일한 PDF 파일의 벡터 스토어 로드 성공!\n",
      "- 벡터 개수: 49\n",
      "- 청크 개수: 49\n",
      "- 원본 파일: 2024-PR-15.pdf\n",
      "- 임베딩 모델: text-embedding-3-large\n",
      "✓ 벡터 스토어 준비 완료!\n",
      "이제 검색을 할 수 있습니다!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# OpenAI API 키 설정 (환경변수에서 읽어오거나 직접 설정)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY  # 필요한 경우\n",
    "\n",
    "def save_vectorstore_openai(index, chunks, metadatas, pdf_filename, save_name=\"pdf_openai_vectors\"):\n",
    "    \"\"\"벡터 스토어 저장\"\"\"\n",
    "    try:\n",
    "        # FAISS 인덱스 저장\n",
    "        faiss.write_index(index, f'{save_name}_vectors.index')\n",
    "        \n",
    "        # 청크와 메타데이터 저장\n",
    "        with open(f'{save_name}_data.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'chunks': chunks,\n",
    "                'metadatas': metadatas,\n",
    "                'pdf_filename': pdf_filename,\n",
    "                'chunk_count': len(chunks),\n",
    "                'embedding_model': 'text-embedding-3-large',\n",
    "                'embedding_type': 'openai'\n",
    "            }, f)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"저장 오류: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_faiss_vectorstore_openai(chunks, pdf_filename, save_name=\"pdf_openai_vectors\"):\n",
    "    \"\"\"청크들을 FAISS 벡터 스토어로 변환하고 자동 저장 (OpenAI 임베딩 사용)\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"청크가 없습니다.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    print(\"OpenAI 임베딩 모델 로드 중...\")\n",
    "    embedding = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "    \n",
    "    print(f\"임베딩 생성 중... ({len(chunks)}개 청크)\")\n",
    "    print(\"OpenAI API 호출 중이므로 시간이 걸릴 수 있습니다...\")\n",
    "    \n",
    "    try:\n",
    "        # 청크들을 임베딩으로 변환\n",
    "        embeddings = embedding.embed_documents(chunks)\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        print(f\"임베딩 완료! 차원: {embeddings.shape[1]}, 개수: {embeddings.shape[0]}\")\n",
    "        \n",
    "        # FAISS 인덱스 생성\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)  # L2 거리 기반 인덱스\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        # 메타데이터 생성\n",
    "        metadatas = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            metadatas.append({\n",
    "                'chunk_id': i,\n",
    "                'source': pdf_filename,\n",
    "                'chunk_size': len(chunk),\n",
    "                'preview': chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
    "            })\n",
    "        \n",
    "        print(f\"FAISS 벡터 스토어 생성 완료! {index.ntotal}개 벡터 저장됨\")\n",
    "        \n",
    "        # 자동 저장\n",
    "        print(\"벡터 스토어를 자동 저장 중...\")\n",
    "        if save_vectorstore_openai(index, chunks, metadatas, pdf_filename, save_name):\n",
    "            print(\"✓ 자동 저장 완료!\")\n",
    "        else:\n",
    "            print(\"⚠️  자동 저장 실패\")\n",
    "        \n",
    "        return index, embedding, chunks, metadatas\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"임베딩 생성 오류: {e}\")\n",
    "        print(\"OpenAI API 키가 설정되어 있는지 확인해주세요.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def load_or_create_vectorstore(chunks, pdf_filename, save_name=\"pdf_openai_vectors\"):\n",
    "    \"\"\"\n",
    "    벡터 스토어가 있으면 로드하고, 없으면 새로 생성하는 함수\n",
    "    \"\"\"\n",
    "    index_file = f'{save_name}_vectors.index'\n",
    "    data_file = f'{save_name}_data.pkl'\n",
    "    \n",
    "    # 저장된 파일들이 모두 존재하는지 확인\n",
    "    if os.path.exists(index_file) and os.path.exists(data_file):\n",
    "        print(\"기존 벡터 스토어를 찾았습니다. 로드 중...\")\n",
    "        \n",
    "        try:\n",
    "            # 저장된 데이터 로드\n",
    "            index = faiss.read_index(index_file)\n",
    "            \n",
    "            with open(data_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            # 저장된 PDF 파일명과 현재 파일명 비교\n",
    "            if data['pdf_filename'] == pdf_filename:\n",
    "                print(\"✓ 동일한 PDF 파일의 벡터 스토어 로드 성공!\")\n",
    "                \n",
    "                # OpenAI 임베딩 모델 로드\n",
    "                embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "                \n",
    "                print(f\"- 벡터 개수: {index.ntotal}\")\n",
    "                print(f\"- 청크 개수: {data['chunk_count']}\")\n",
    "                print(f\"- 원본 파일: {data['pdf_filename']}\")\n",
    "                print(f\"- 임베딩 모델: {data.get('embedding_model', '정보 없음')}\")\n",
    "                \n",
    "                return index, embedding_model, data['chunks'], data['metadatas']\n",
    "            else:\n",
    "                print(f\"⚠️  다른 PDF 파일의 벡터 스토어입니다.\")\n",
    "                print(f\"   저장된 파일: {data['pdf_filename']}\")\n",
    "                print(f\"   현재 파일: {pdf_filename}\")\n",
    "                print(\"   새로운 벡터 스토어를 생성합니다...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  벡터 스토어 로드 중 오류 발생: {e}\")\n",
    "            print(\"   새로운 벡터 스토어를 생성합니다...\")\n",
    "    \n",
    "    else:\n",
    "        print(\"저장된 벡터 스토어를 찾을 수 없습니다. 새로 생성합니다...\")\n",
    "    \n",
    "    # 새로운 벡터 스토어 생성\n",
    "    print(\"\\n=== 새로운 벡터 스토어 생성 ===\")\n",
    "    return create_faiss_vectorstore_openai(chunks, pdf_filename, save_name)\n",
    "\n",
    "# FAISS 벡터 스토어 자동 로드/생성\n",
    "if 'chunks' in locals() and chunks:\n",
    "    print(\"=== 벡터 스토어 처리 ===\")\n",
    "    index, embedding_model, chunks, metadatas = load_or_create_vectorstore(\n",
    "        chunks, \n",
    "        pdf_file, \n",
    "        save_name=\"my_pdf_vectors\"  # 원하는 저장 이름으로 변경 가능\n",
    "    )\n",
    "    \n",
    "    if index is not None:\n",
    "        print(\"✓ 벡터 스토어 준비 완료!\")\n",
    "        print(\"이제 검색을 할 수 있습니다!\")\n",
    "    else:\n",
    "        print(\"✗ 벡터 스토어 준비 실패\")\n",
    "else:\n",
    "    print(\"청크가 없습니다. 먼저 텍스트 추출과 청킹을 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e708b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 웹 검색 도구 준비 완료\n"
     ]
    }
   ],
   "source": [
    "# cell [30] 교체\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 웹 검색 도구 초기화\n",
    "try:\n",
    "    search_tool = DuckDuckGoSearchRun()\n",
    "    print(\"✅ 웹 검색 도구 준비 완료\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 웹 검색 도구 초기화 실패: {e}\")\n",
    "    search_tool = None\n",
    "\n",
    "@tool\n",
    "def search_uploaded_documents(query: str, k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    업로드된 문서에서 내용을 검색합니다.\n",
    "    \"\"\"\n",
    "    global index, embedding_model, chunks, metadatas\n",
    "    \n",
    "    if 'index' not in globals() or index is None:\n",
    "        return \"업로드된 문서가 없습니다. 먼저 파일을 업로드해주세요.\"\n",
    "    \n",
    "    try:\n",
    "        results = search_documents_openai(query, index, embedding_model, chunks, metadatas, k)\n",
    "        \n",
    "        if not results:\n",
    "            return f\"'{query}'에 대한 검색 결과가 없습니다.\"\n",
    "        \n",
    "        formatted_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            formatted_results.append(\n",
    "                f\"[문서 검색결과 {i+1}]\\n\"\n",
    "                f\"유사도: {result['score']:.3f}\\n\"\n",
    "                f\"내용: {result['content'][:400]}{'...' if len(result['content']) > 400 else ''}\\n\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"문서 검색 중 오류: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    웹에서 최신 정보를 검색합니다.\n",
    "    \"\"\"\n",
    "    if search_tool is None:\n",
    "        return \"웹 검색 기능을 사용할 수 없습니다.\"\n",
    "    \n",
    "    try:\n",
    "        results = search_tool.run(query)\n",
    "        return f\"🌐 웹 검색 결과:\\n{results}\"\n",
    "    except Exception as e:\n",
    "        return f\"웹 검색 중 오류 발생: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def summarize_content(content: str, style: str = \"general\") -> str:\n",
    "    \"\"\"\n",
    "    주어진 내용을 요약합니다.\n",
    "    \"\"\"\n",
    "    if not content.strip():\n",
    "        return \"요약할 내용이 없습니다.\"\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "    \n",
    "    style_prompts = {\n",
    "        \"general\": \"다음 내용을 자연스럽게 요약해주세요:\",\n",
    "        \"bullet_points\": \"다음 내용을 주요 포인트별로 불릿 포인트로 요약해주세요:\",\n",
    "        \"executive\": \"다음 내용을 경영진을 위한 간단한 요약으로 작성해주세요:\",\n",
    "        \"detailed\": \"다음 내용을 상세하게 요약하되 중요한 내용은 빠뜨리지 마세요:\"\n",
    "    }\n",
    "    \n",
    "    prompt = style_prompts.get(style, style_prompts[\"general\"])\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke([\n",
    "            HumanMessage(content=f\"{prompt}\\n\\n{content}\")\n",
    "        ])\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"요약 중 오류 발생: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def get_document_info() -> str:\n",
    "    \"\"\"\n",
    "    현재 로드된 PDF 문서의 정보를 반환합니다.\n",
    "    \"\"\"\n",
    "    global chunks, metadatas, pdf_file\n",
    "    \n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        return \"로드된 문서가 없습니다.\"\n",
    "    \n",
    "    total_chars = sum(len(chunk) for chunk in chunks)\n",
    "    \n",
    "    return f\"\"\"\n",
    "📄 현재 로드된 문서 정보:\n",
    "- 파일명: {pdf_file if 'pdf_file' in globals() else '알 수 없음'}\n",
    "- 총 청크 수: {len(chunks)}\n",
    "- 총 글자 수: {total_chars:,}\n",
    "- 평균 청크 크기: {total_chars // len(chunks):,} 글자\n",
    "\n",
    "🛠️ 사용 가능한 기능:\n",
    "- 문서 검색: \"교통약자에 대해 찾아줘\"\n",
    "- 웹 검색: \"최신 AI 뉴스 검색해줘\"  \n",
    "- 내용 요약: \"이 문서를 요약해줘\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "88c62b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 셀 추가\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def enhanced_call_model(state: AgentState) -> AgentState:\n",
    "    \"\"\"향상된 LLM 모델 호출\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "당신은 다기능 AI 어시스턴트입니다. 다음 기능들을 제공할 수 있습니다:\n",
    "\n",
    "🔍 **검색 기능**:\n",
    "- search_uploaded_documents: 업로드된 PDF 문서 검색\n",
    "- web_search: 최신 웹 정보 검색\n",
    "\n",
    "📝 **분석 기능**:\n",
    "- summarize_content: 다양한 스타일로 내용 요약\n",
    "- get_document_info: 현재 문서 상태 확인\n",
    "\n",
    "사용자 요청에 따라 적절한 도구를 선택하세요:\n",
    "\n",
    "1. 업로드된 문서에 대한 질문 → search_uploaded_documents 사용\n",
    "2. 최신 정보나 일반 질문 → web_search 사용  \n",
    "3. 요약 요청 → summarize_content 사용\n",
    "4. 문서 정보 요청 → get_document_info 사용\n",
    "\n",
    "항상 한국어로 친절하고 정확하게 답변해주세요.\n",
    "\"\"\"\n",
    "    \n",
    "    if not messages or \"system\" not in str(messages[0]):\n",
    "        messages = [HumanMessage(content=system_prompt)] + messages\n",
    "    \n",
    "    # 모든 도구 바인딩\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.1)\n",
    "    tools = [search_uploaded_documents, web_search, summarize_content, get_document_info]\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    \n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\"messages\": messages + [response]}\n",
    "\n",
    "# 새로운 도구 노드\n",
    "tools = [search_uploaded_documents, web_search, summarize_content, get_document_info]\n",
    "enhanced_tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f32b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_document_info() -> str:\n",
    "    \"\"\"\n",
    "    현재 로드된 PDF 문서의 정보를 반환합니다.\n",
    "    \n",
    "    Returns:\n",
    "        문서 정보\n",
    "    \"\"\"\n",
    "    global chunks, metadatas, pdf_file\n",
    "    \n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        return \"로드된 문서가 없습니다.\"\n",
    "    \n",
    "    total_chars = sum(len(chunk) for chunk in chunks)\n",
    "    \n",
    "    return f\"\"\"\n",
    "현재 로드된 문서 정보:\n",
    "- 파일명: {pdf_file if 'pdf_file' in globals() else '알 수 없음'}\n",
    "- 총 청크 수: {len(chunks)}\n",
    "- 총 글자 수: {total_chars:,}\n",
    "- 평균 청크 크기: {total_chars // len(chunks):,} 글자\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b7faabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"다음 단계를 결정하는 함수\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # 도구 호출이 있으면 도구 실행\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # 그렇지 않으면 종료\n",
    "    return END\n",
    "\n",
    "def call_model(state: AgentState) -> AgentState:\n",
    "    \"\"\"LLM 모델 호출\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"\n",
    "당신은 PDF 문서 검색 및 요약 전문 에이전트입니다.\n",
    "\n",
    "사용 가능한 도구들:\n",
    "1. search_pdf_documents: PDF에서 관련 내용 검색\n",
    "2. summarize_content: 내용 요약\n",
    "3. get_document_info: 문서 정보 조회\n",
    "\n",
    "사용자의 질문에 따라 적절한 도구를 사용하여 답변하세요:\n",
    "- 특정 내용을 찾고 싶다면 search_pdf_documents를 사용\n",
    "- 검색된 내용을 요약하고 싶다면 summarize_content를 사용\n",
    "- 문서 정보가 궁금하다면 get_document_info를 사용\n",
    "\n",
    "항상 한국어로 친절하고 정확하게 답변해주세요.\n",
    "\"\"\"\n",
    "    \n",
    "    # 시스템 메시지가 없다면 추가\n",
    "    if not messages or not any(isinstance(msg, HumanMessage) for msg in messages[:1]):\n",
    "        messages = [HumanMessage(content=system_prompt)] + messages\n",
    "    \n",
    "    # LLM 호출\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.1)\n",
    "    llm_with_tools = llm.bind_tools([search_pdf_documents, summarize_content, get_document_info])\n",
    "    \n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\"messages\": messages + [response]}\n",
    "\n",
    "def call_tools(state: AgentState) -> AgentState:\n",
    "    \"\"\"도구 실행\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # 도구 실행기 생성\n",
    "    tools = [search_pdf_documents, summarize_content, get_document_info]\n",
    "    tool_executor = ToolExecutor(tools)\n",
    "    \n",
    "    tool_messages = []\n",
    "    \n",
    "    # 각 도구 호출 실행\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        try:\n",
    "            result = tool_executor.invoke(tool_call)\n",
    "            tool_messages.append(\n",
    "                ToolMessage(\n",
    "                    content=str(result),\n",
    "                    tool_call_id=tool_call[\"id\"]\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            tool_messages.append(\n",
    "                ToolMessage(\n",
    "                    content=f\"도구 실행 오류: {str(e)}\",\n",
    "                    tool_call_id=tool_call[\"id\"]\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return {\"messages\": messages + tool_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01c0c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_graph():\n",
    "    \"\"\"LangGraph 에이전트 그래프 생성\"\"\"\n",
    "    try:\n",
    "        # 도구들\n",
    "        tools = [search_uploaded_documents, web_search_tool, summarize_content_tool, get_document_info_tool]\n",
    "        \n",
    "        # ToolNode 사용 (최신 LangGraph)\n",
    "        tool_node = ToolNode(tools)\n",
    "        \n",
    "        # 그래프 빌더\n",
    "        graph_builder = StateGraph(AgentState)\n",
    "        \n",
    "        # 노드 추가\n",
    "        graph_builder.add_node(\"agent\", call_model)\n",
    "        graph_builder.add_node(\"tools\", tool_node)\n",
    "        \n",
    "        # 시작점 설정\n",
    "        graph_builder.set_entry_point(\"agent\")\n",
    "        \n",
    "        # 조건부 엣지 추가\n",
    "        graph_builder.add_conditional_edges(\n",
    "            \"agent\",\n",
    "            should_continue,\n",
    "            {\n",
    "                \"tools\": \"tools\",\n",
    "                END: END\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 도구에서 다시 에이전트로\n",
    "        graph_builder.add_edge(\"tools\", \"agent\")\n",
    "        \n",
    "        # 그래프 컴파일\n",
    "        return graph_builder.compile()\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"에이전트 그래프 생성 오류: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_agent_with_graph(user_query: str) -> str:\n",
    "    \"\"\"LangGraph 에이전트 실행\"\"\"\n",
    "    try:\n",
    "        # 그래프가 없으면 생성\n",
    "        if st.session_state.agent_graph is None:\n",
    "            st.session_state.agent_graph = create_agent_graph()\n",
    "        \n",
    "        if st.session_state.agent_graph is None:\n",
    "            return \"에이전트 그래프 생성에 실패했습니다.\"\n",
    "        \n",
    "        # 초기 상태 설정\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=user_query)]\n",
    "        }\n",
    "        \n",
    "        # 에이전트 실행\n",
    "        result = st.session_state.agent_graph.invoke(initial_state)\n",
    "        final_message = result[\"messages\"][-1]\n",
    "        \n",
    "        if hasattr(final_message, 'content'):\n",
    "            return final_message.content\n",
    "        else:\n",
    "            return str(final_message)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"에이전트 실행 중 오류: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "868fb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 처리 함수\n",
    "def process_file(uploaded_file):\n",
    "    \"\"\"업로드된 파일 처리\"\"\"\n",
    "    if uploaded_file is None:\n",
    "        return False, \"파일을 업로드해주세요.\"\n",
    "    \n",
    "    if not uploaded_file.name.endswith('.pdf'):\n",
    "        return False, \"PDF 파일만 지원됩니다.\"\n",
    "    \n",
    "    try:\n",
    "        # 임시 파일에 저장\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "            tmp_file.write(uploaded_file.getvalue())\n",
    "            tmp_path = tmp_file.name\n",
    "        \n",
    "        # 텍스트 추출\n",
    "        with st.spinner(\"PDF 텍스트 추출 중...\"):\n",
    "            full_text = extract_text_with_fitz(tmp_path)\n",
    "        \n",
    "        # 임시 파일 삭제\n",
    "        os.unlink(tmp_path)\n",
    "        \n",
    "        if not full_text or len(full_text.strip()) < 10:\n",
    "            return False, \"파일에서 텍스트를 추출할 수 없었습니다.\"\n",
    "        \n",
    "        # 텍스트 청킹\n",
    "        with st.spinner(\"텍스트 청킹 중...\"):\n",
    "            chunks = chunk_text(full_text)\n",
    "        \n",
    "        if not chunks:\n",
    "            return False, \"텍스트 청킹에 실패했습니다.\"\n",
    "        \n",
    "        # 벡터 스토어 생성\n",
    "        index, embedding_model, metadatas = create_vectorstore(chunks)\n",
    "        \n",
    "        if index is None:\n",
    "            return False, \"벡터 스토어 생성에 실패했습니다.\"\n",
    "        \n",
    "        # 세션 상태 업데이트\n",
    "        st.session_state.chunks = chunks\n",
    "        st.session_state.index = index\n",
    "        st.session_state.embedding_model = embedding_model\n",
    "        st.session_state.metadatas = metadatas\n",
    "        st.session_state.file_processed = True\n",
    "        \n",
    "        # 기존 그래프 초기화 (새 문서용)\n",
    "        st.session_state.agent_graph = None\n",
    "        \n",
    "        return True, f\"파일 처리 완료! 총 {len(chunks)}개 청크로 분할되었습니다.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"파일 처리 중 오류 발생: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5b32321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 향상된 멀티모달 에이전트 준비 완료!\n",
      "🔍 웹 검색 + 📄 문서 검색 + 📝 요약 기능 모두 사용 가능\n"
     ]
    }
   ],
   "source": [
    "# 향상된 그래프 생성\n",
    "enhanced_graph = StateGraph(AgentState)\n",
    "\n",
    "enhanced_graph.add_node(\"agent\", enhanced_call_model)\n",
    "enhanced_graph.add_node(\"tools\", enhanced_tool_node)\n",
    "\n",
    "enhanced_graph.set_entry_point(\"agent\")\n",
    "enhanced_graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "enhanced_graph.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph = enhanced_graph.compile()\n",
    "\n",
    "print(\"✅ 향상된 멀티모달 에이전트 준비 완료!\")\n",
    "print(\"🔍 웹 검색 + 📄 문서 검색 + 📝 요약 기능 모두 사용 가능\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1c093230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_enhanced_agent(user_query: str) -> str:\n",
    "    \"\"\"향상된 에이전트 실행\"\"\"\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=user_query)],\n",
    "        \"user_query\": user_query,\n",
    "        \"search_results\": [],\n",
    "        \"summary\": \"\",\n",
    "        \"tool_calls\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = graph.invoke(initial_state)  # graph 사용 (이미 정의됨)\n",
    "        final_message = result[\"messages\"][-1]\n",
    "        \n",
    "        if hasattr(final_message, 'content'):\n",
    "            return final_message.content\n",
    "        else:\n",
    "            return str(final_message)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"에이전트 실행 중 오류 발생: {str(e)}\"\n",
    "\n",
    "# pdf_agent는 graph로 대체\n",
    "pdf_agent = graph\n",
    "\n",
    "def interactive_pdf_agent():\n",
    "    \"\"\"대화형 PDF 에이전트\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🤖 멀티모달 AI 어시스턴트\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"• '종료', 'quit', 'exit'를 입력하면 종료됩니다\")\n",
    "    print(\"• 예시 질문:\")\n",
    "    print(\"  - '교통약자에 대해 검색해줘'\")\n",
    "    print(\"  - '이 문서를 요약해줘'\")\n",
    "    print(\"  - '최신 AI 뉴스 검색해줘'\")\n",
    "    print(\"  - '문서 정보를 알려줘'\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\n💬 질문을 입력하세요: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['종료', 'quit', 'exit', 'q']:\n",
    "                print(\"에이전트를 종료합니다. 👋\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            print(\"\\n🤖 에이전트가 작업 중입니다...\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # 향상된 에이전트 실행\n",
    "            response = run_enhanced_agent(user_input)\n",
    "            \n",
    "            print(f\"\\n🎯 답변:\\n{response}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n에이전트를 종료합니다. 👋\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5285c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 벡터 스토어가 준비되었는지 확인\n",
    "# if 'index' in locals() and index is not None:\n",
    "#     print(\"✅ 멀티모달 AI 어시스턴트가 준비되었습니다!\")\n",
    "    \n",
    "#     # 테스트 실행\n",
    "#     test_query = \"문서 정보를 알려줘\"\n",
    "#     print(f\"\\n🧪 테스트 실행: {test_query}\")\n",
    "#     result = run_enhanced_agent(test_query)\n",
    "#     print(f\"결과: {result[:200]}...\")\n",
    "    \n",
    "#     # 대화형 실행\n",
    "#     print(\"\\n🚀 대화형 에이전트를 시작하시겠습니까? (y/n)\")\n",
    "#     if input(\"입력: \").lower() in ['y', 'yes', '네', 'ㅇ']:\n",
    "#         interactive_pdf_agent()\n",
    "        \n",
    "# else:\n",
    "#     print(\"❌ 벡터 스토어가 준비되지 않았습니다.\")\n",
    "#     print(\"먼저 PDF 처리와 벡터 스토어 생성을 완료해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7afcf00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_safe_agent(user_query: str) -> str:\n",
    "    \"\"\"안전한 에이전트 실행 - 에러 방지\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 간단한 직접 응답 케이스들\n",
    "        if \"안녕\" in user_query or \"hello\" in user_query.lower():\n",
    "            return \"안녕하세요! 저는 멀티모달 AI 어시스턴트입니다. 문서 검색, 웹 검색, 요약 등을 도와드릴 수 있어요. 무엇을 도와드릴까요?\"\n",
    "        \n",
    "        # 도구가 필요한 경우들\n",
    "        if any(keyword in user_query for keyword in [\"문서\", \"정보\", \"알려\", \"검색\", \"요약\", \"찾아\"]):\n",
    "            \n",
    "            # 문서 정보 요청\n",
    "            if \"문서\" in user_query and \"정보\" in user_query:\n",
    "                return get_document_info().invoke({})\n",
    "            \n",
    "            # 문서 검색 요청  \n",
    "            elif any(keyword in user_query for keyword in [\"교통\", \"고령\", \"약자\", \"정책\"]):\n",
    "                if 'index' in globals() and index is not None:\n",
    "                    return search_uploaded_documents.invoke({\"query\": user_query})\n",
    "                else:\n",
    "                    return \"문서가 업로드되지 않았습니다. 먼저 PDF 파일을 업로드해주세요.\"\n",
    "            \n",
    "            # 웹 검색 요청\n",
    "            elif any(keyword in user_query for keyword in [\"최신\", \"뉴스\", \"날씨\", \"AI\"]):\n",
    "                return web_search.invoke({\"query\": user_query})\n",
    "            \n",
    "            # 요약 요청\n",
    "            elif \"요약\" in user_query:\n",
    "                if 'chunks' in globals() and chunks:\n",
    "                    # 전체 문서의 일부를 요약\n",
    "                    sample_content = \"\\n\".join(chunks[:3])  # 처음 3개 청크\n",
    "                    return summarize_content.invoke({\"content\": sample_content})\n",
    "                else:\n",
    "                    return \"요약할 문서가 없습니다. 먼저 PDF 파일을 업로드해주세요.\"\n",
    "        \n",
    "        # 기본 LLM 응답\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "        response = llm.invoke([HumanMessage(content=user_query)])\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"죄송합니다. 처리 중 오류가 발생했습니다: {str(e)}\\n\\n다른 질문을 시도해보시거나, 문서를 먼저 업로드해주세요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c8677128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 10:11:05.063 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.064 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.066 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2025-06-20 10:11:05.066 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.066 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.067 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.067 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.068 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.068 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.069 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.069 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.069 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.070 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.070 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.071 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.071 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.071 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.071 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.072 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.072 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.072 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.073 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.073 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.073 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.108 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\AI_Prompt\\workspace\\ai_agent_work2\\mini_summary\\mini\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-06-20 10:11:05.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.112 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.112 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.112 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.119 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.119 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:11:05.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# ============== Streamlit UI ===============\n",
    "import streamlit as st\n",
    "st.set_page_config(\n",
    "    page_title=\"🤖 LangGraph AI 어시스턴트\",\n",
    "    page_icon=\"🤖\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "def main():\n",
    "    initialize_session_state()\n",
    "    \n",
    "    # 헤더\n",
    "    st.markdown('<h1 style=\"text-align: center; color: #1f77b4;\">🤖 LangGraph AI 어시스턴트</h1>', unsafe_allow_html=True)\n",
    "    st.markdown('<p style=\"text-align: center; color: #666;\">LangGraph Tools를 활용한 멀티모달 문서 어시스턴트</p>', unsafe_allow_html=True)\n",
    "    \n",
    "    # 사이드바\n",
    "    with st.sidebar:\n",
    "        st.header(\"📁 파일 업로드\")\n",
    "        \n",
    "        uploaded_file = st.file_uploader(\n",
    "            \"PDF 파일을 선택하세요\",\n",
    "            type=['pdf'],\n",
    "            help=\"PDF 파일을 업로드하면 LangGraph 도구들을 사용하여 문서를 분석할 수 있습니다.\"\n",
    "        )\n",
    "        \n",
    "        if uploaded_file is not None:\n",
    "            if st.button(\"파일 처리\", type=\"primary\"):\n",
    "                success, message = process_file(uploaded_file)\n",
    "                if success:\n",
    "                    st.success(f\"✅ {message}\")\n",
    "                else:\n",
    "                    st.error(f\"❌ {message}\")\n",
    "        \n",
    "        # 상태 표시\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"📊 시스템 상태\")\n",
    "        if st.session_state.file_processed:\n",
    "            st.success(\"✅ 문서 처리 완료\")\n",
    "            st.info(f\"📄 청크 수: {len(st.session_state.chunks)}\")\n",
    "            st.info(f\"🔧 LangGraph: {'활성화' if st.session_state.agent_graph else '대기 중'}\")\n",
    "        else:\n",
    "            st.warning(\"⏳ 문서 미처리\")\n",
    "        \n",
    "        # LangGraph 도구 정보\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"🛠️ 사용 가능한 도구들\")\n",
    "        st.markdown(\"\"\"\n",
    "        **LangGraph Tools:**\n",
    "        - `search_uploaded_documents` 📄\n",
    "        - `web_search_tool` 🌐  \n",
    "        - `summarize_content_tool` 📝\n",
    "        - `get_document_info_tool` ℹ️\n",
    "        \"\"\")\n",
    "        \n",
    "        # 빠른 명령어\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"🚀 빠른 테스트\")\n",
    "        \n",
    "        if st.button(\"📄 문서 정보\"):\n",
    "            test_msg = {\"role\": \"user\", \"content\": \"문서 정보를 알려줘\"}\n",
    "            st.session_state.messages.append(test_msg)\n",
    "            response = run_agent_with_graph(\"문서 정보를 알려줘\")\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "            st.rerun()\n",
    "        \n",
    "        if st.button(\"🔍 문서 검색\"):\n",
    "            test_msg = {\"role\": \"user\", \"content\": \"교통약자에 대해 검색해줘\"}\n",
    "            st.session_state.messages.append(test_msg)\n",
    "            response = run_agent_with_graph(\"교통약자에 대해 검색해줘\")\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "            st.rerun()\n",
    "    \n",
    "    # 메인 채팅 영역\n",
    "    st.header(\"💬 LangGraph 에이전트와 대화\")\n",
    "    \n",
    "    # 채팅 기록\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "    \n",
    "    # 채팅 입력\n",
    "    if prompt := st.chat_input(\"LangGraph 도구들을 테스트해보세요...\"):\n",
    "        # 사용자 메시지\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "        \n",
    "        # 에이전트 응답\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            with st.spinner(\"LangGraph 에이전트가 도구들을 사용하여 응답 생성 중...\"):\n",
    "                response = run_agent_with_graph(prompt)\n",
    "            st.markdown(response)\n",
    "        \n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    # 기록 초기화\n",
    "    if st.button(\"🗑️ 채팅 기록 초기화\"):\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
