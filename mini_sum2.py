# ============== PDF ë¶„ì„ Streamlit ì•± ==============
import streamlit as st
import fitz  # pymupdf
import faiss
import numpy as np
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from typing import TypedDict, List, Dict, Any
import pickle
import os
import tempfile
from dotenv import load_dotenv
import time

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# í™˜ê²½ ì„¤ì •
load_dotenv()

# ============== ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ==============
def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”"""
    if 'chunks' not in st.session_state:
        st.session_state.chunks = []
    if 'index' not in st.session_state:
        st.session_state.index = None
    if 'embedding_model' not in st.session_state:
        st.session_state.embedding_model = None
    if 'metadatas' not in st.session_state:
        st.session_state.metadatas = []
    if 'file_processed' not in st.session_state:
        st.session_state.file_processed = False
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'user_input_text' not in st.session_state: # text_inputì˜ keyì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        st.session_state.user_input_text = ""
    if 'last_processed_file_name' not in st.session_state:
        st.session_state.last_processed_file_name = None
    # Langgraph ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    if 'agent_graph' not in st.session_state:
        st.session_state.agent_graph = None

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜ í˜¸ì¶œ
init_session_state()

# ============== Langchain ì„¤ì • ==============
# OpenAI API í‚¤ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    st.error("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# LLM ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ì„¸ì…˜ ìƒíƒœì— ì €ì¥)
@st.cache_resource
def get_llm():
    return ChatOpenAI(model="gpt-4o", temperature=0.3)

@st.cache_resource
def get_embedding_model():
    return OpenAIEmbeddings(model="text-embedding-3-large")

st.session_state.llm = get_llm()
st.session_state.embedding_model = get_embedding_model()

# ============== FAISS ì¸ë±ì‹± ë° ê²€ìƒ‰ í•¨ìˆ˜ ==============
def create_faiss_index(chunks, metadatas):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë“¤ë¡œ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not chunks:
        return None
    try:
        embeddings = st.session_state.embedding_model.embed_documents(chunks)
        index = faiss.IndexFlatL2(len(embeddings[0]))
        index.add(np.array(embeddings).astype('float32'))
        return index
    except Exception as e:
        st.error(f"FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def search_faiss_index(query, index, chunks, metadatas, k=3):
    """FAISS ì¸ë±ìŠ¤ì—ì„œ ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ kê°œì˜ ë©ì–´ë¦¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    if index is None:
        return []
    try:
        query_embedding = st.session_state.embedding_model.embed_query(query)
        D, I = index.search(np.array([query_embedding]).astype('float32'), k)
        results = []
        for i in I[0]:
            if i != -1:  # ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ì¶”ê°€
                results.append({"content": chunks[i], "metadata": metadatas[i]})
        return results
    except Exception as e:
        st.error(f"FAISS ì¸ë±ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

# ============== í…ìŠ¤íŠ¸ ë¶„í•  í•¨ìˆ˜ ==============
def get_text_chunks(text, chunk_size=1000, chunk_overlap=200):
    """í…ìŠ¤íŠ¸ë¥¼ ë©ì–´ë¦¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„í•  ë¡œì§ (ê°œì„ ì„ ìœ„í•´ Langchainì˜ RecursiveCharacterTextSplitter ì‚¬ìš© ê³ ë ¤)
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size - chunk_overlap
    return chunks

# ============== PDF ì²˜ë¦¬ í•¨ìˆ˜ ==============
def process_pdf(uploaded_file):
    """PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if uploaded_file is None:
        return [], []

    # íŒŒì¼ ì´ë¦„ì´ ë°”ë€Œì§€ ì•Šì•˜ê³  ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ë‹¤ì‹œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
    if (st.session_state.file_processed and
        st.session_state.last_processed_file_name == uploaded_file.name):
        return st.session_state.chunks, st.session_state.metadatas

    try:
        # Streamlitì˜ UploadedFile ê°ì²´ì—ì„œ ì§ì ‘ íŒŒì¼ ë‚´ìš© ì½ê¸°
        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        full_text = ""
        page_texts = []
        page_metadatas = []

        for page_num in range(len(pdf_document)):
            page = pdf_document.load_page(page_num)
            text = page.get_text()
            full_text += text
            page_texts.append(text) # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥

            # ê°„ë‹¨í•œ ë©”íƒ€ë°ì´í„°
            page_metadatas.append({"page_number": page_num + 1, "source": uploaded_file.name})

        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë©ì–´ë¦¬ë¡œ ë¶„í•  (RAG ì‹œìŠ¤í…œì„ ìœ„í•´)
        chunks = get_text_chunks(full_text)
        metadatas_for_chunks = []
        # ê° ë©ì–´ë¦¬ì— í•´ë‹¹í•˜ëŠ” í˜ì´ì§€ ì •ë³´ ì¶”ì • (ê°„ë‹¨í™”)
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°ê°€ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ë„ë¡ í•´ì•¼ í•¨
        for i, chunk in enumerate(chunks):
            metadatas_for_chunks.append({"chunk_index": i, "source": uploaded_file.name})

        st.session_state.chunks = chunks
        st.session_state.metadatas = metadatas_for_chunks
        st.session_state.file_processed = True
        st.session_state.last_processed_file_name = uploaded_file.name
        return chunks, metadatas_for_chunks
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        st.session_state.file_processed = False
        st.session_state.last_processed_file_name = None
        return [], []

# ============== Langgraph íˆ´ ì •ì˜ ë° ê·¸ë˜í”„ ìƒì„± ==============
@tool
def pdf_search_tool(query: str):
    """
    ì—…ë¡œë“œëœ PDF ë¬¸ì„œì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ íŠ¹ì • ë¬¸ì„œ ë‚´ìš©ì„ ìš”êµ¬í•  ë•Œ ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    ì˜ˆì‹œ: "ì´ ë¬¸ì„œì—ì„œ ì£¼ìš” ë‚´ìš©ì€ ë­ì•¼?", "PDFì— ë”°ë¥´ë©´ Aì— ëŒ€í•œ ì •ë³´ëŠ” ë­ì•¼?"
    """
    if st.session_state.index is None:
        return "PDF íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì¸ë±ìŠ¤ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    results = search_faiss_index(query, st.session_state.index, st.session_state.chunks, st.session_state.metadatas)
    
    if not results:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. PDFì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ê²€ìƒ‰ëœ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ë°˜í™˜
    context = "\n\n".join([r["content"] for r in results])
    
    # LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
    prompt = f"""
    ë‹¤ìŒì€ PDF ë¬¸ì„œì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:
    ---
    {context}
    ---
    ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.
    ì§ˆë¬¸: {query}
    """
    
    try:
        response = st.session_state.llm.invoke(prompt)
        return response.content
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

@tool
def general_chat_tool(query: str):
    """
    ì—…ë¡œë“œëœ PDFì™€ ê´€ë ¨ ì—†ëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ë‹µí•˜ê±°ë‚˜, PDF ê²€ìƒ‰ ê²°ê³¼ë¡œ ì¶©ë¶„í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ì´ ë„êµ¬ëŠ” ì¼ë°˜ ìƒì‹, ì •ì˜, ê°œë… ì„¤ëª… ë“± ë„“ì€ ë²”ìœ„ì˜ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì˜ˆì‹œ: "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?", "íŒŒì´ì¬ì´ë€?", "ì¸ê³µì§€ëŠ¥ì˜ ì¢…ë¥˜ëŠ”?"
    """
    try:
        response = st.session_state.llm.invoke(query)
        return response.content
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# Langgraph ë…¸ë“œ ì •ì˜
def call_model(state):
    messages = state["messages"]
    last_message = messages[-1]

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ToolMessageì¸ ê²½ìš°, íˆ´ ì¶œë ¥ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
    if isinstance(last_message, ToolMessage):
        # íˆ´ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µ ìƒì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        tool_output = last_message.content
        human_message_content = ""
        for msg in messages:
            if isinstance(msg, HumanMessage):
                human_message_content = msg.content
                break # ì²« ë²ˆì§¸ HumanMessageë¥¼ ì°¾ìœ¼ë©´ ì¢…ë£Œ

        prompt = f"""
        ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸: {human_message_content}
        ë„êµ¬ ê²€ìƒ‰ ê²°ê³¼: {tool_output}
        
        ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        """
        response = st.session_state.llm.invoke(prompt)
        return {"messages": messages + [AIMessage(content=response.content)]}
    else:
        # AI ëª¨ë¸ì´ íˆ´ ì‚¬ìš© ì—¬ë¶€ë¥¼ ê²°ì •í•˜ë„ë¡ í•¨
        response = st.session_state.llm.invoke(messages)
        return {"messages": messages + [response]}

# Langgraph ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    messages: List[Any]

# Langgraph ê·¸ë˜í”„ êµ¬ì¶• í•¨ìˆ˜
@st.cache_resource
def create_agent_graph():
    workflow = StateGraph(AgentState)

    workflow.add_node("call_model", call_model)
    workflow.add_node("tool_node", ToolNode(tools=[pdf_search_tool, general_chat_tool]))

    workflow.set_entry_point("call_model")

    def route_tools(state):
        last_message = state["messages"][-1]
        if last_message.tool_calls:
            return "tool_node"
        return END

    workflow.add_conditional_edges(
        "call_model",
        route_tools,
        {"tool_node": "tool_node", END: END}
    )
    workflow.add_edge("tool_node", "call_model") # íˆ´ ì‹¤í–‰ í›„ ë‹¤ì‹œ ëª¨ë¸ í˜¸ì¶œ (ì„ íƒì )
    return workflow.compile()

# ì•± ì´ˆê¸°í™” ì‹œ ê·¸ë˜í”„ ìƒì„±
if st.session_state.agent_graph is None:
    st.session_state.agent_graph = create_agent_graph()

# ============== Streamlit UI ==============
st.title("ğŸ“š PDF ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸")
st.write("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ë©´ AIê°€ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")

with st.sidebar:
    st.header("PDF íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—¬ê¸°ì— ë“œë˜ê·¸ ì•¤ ë“œë¡­í•˜ê±°ë‚˜ í´ë¦­í•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")

    if uploaded_file:
        st.write(f"ì—…ë¡œë“œëœ íŒŒì¼: {uploaded_file.name}")
        with st.spinner("PDFë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
            chunks, metadatas = process_pdf(uploaded_file)
            if chunks:
                st.session_state.index = create_faiss_index(chunks, metadatas)
                if st.session_state.index:
                    st.success("PDF ì²˜ë¦¬ ë° ì¸ë±ì‹± ì™„ë£Œ!")
                else:
                    st.error("FAISS ì¸ë±ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.session_state.file_processed = False
        st.session_state.last_processed_file_name = None
        st.session_state.chunks = []
        st.session_state.metadatas = []
        st.session_state.index = None
        # PDF íŒŒì¼ì´ ì—†ì„ ê²½ìš° ê¸°ì¡´ ì±„íŒ… ê¸°ë¡ì„ ì´ˆê¸°í™” (ì„ íƒ ì‚¬í•­)
        # st.session_state.chat_history = []
        st.info("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”.")

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ (st.chat_message ì‚¬ìš©)
st.subheader("ëŒ€í™” ê¸°ë¡")
for role, message in st.session_state.chat_history:
    with st.chat_message(role):
        st.write(message)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ í•¨ìˆ˜
def handle_query_submission():
    current_input = st.session_state.user_input_text.strip()
    if not current_input:
        return # ì…ë ¥ê°’ì´ ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€ ë° UI í‘œì‹œ
    st.session_state.chat_history.append(("user", current_input))
    
    # AI ì‘ë‹µ ìƒì„±
    with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        try:
            # Langgraph ì‹¤í–‰
            initial_state = {"messages": [HumanMessage(content=current_input)]}
            result = st.session_state.agent_graph.invoke(initial_state) # invokeë¡œ ë³€ê²½
            
            # ì‘ë‹µ ì¶”ì¶œ
            if result and "messages" in result:
                final_message = result["messages"][-1]
                response = final_message.content if hasattr(final_message, 'content') else str(final_message)
            else:
                response = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # AI ë©”ì‹œì§€ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
            st.session_state.chat_history.append(("assistant", response))
            
        except Exception as e:
            error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            st.error(error_msg)
            st.session_state.chat_history.append(("assistant", error_msg))
    
    # ì…ë ¥ì°½ ì´ˆê¸°í™”
    st.session_state.user_input_text = ""
    # ì´ ì½œë°± í•¨ìˆ˜ê°€ ì™„ë£Œë˜ë©´ Streamlitì´ ìë™ìœ¼ë¡œ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¬ì‹¤í–‰í•˜ì—¬ UIë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    # ë”°ë¼ì„œ ëª…ì‹œì ì¸ st.rerun()ì€ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°, ì˜¤íˆë ¤ ì¤‘ë³µ ì¬ì‹¤í–‰ì„ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# ì‚¬ìš©ì ì…ë ¥ í•„ë“œ (Enter í‚¤ ì…ë ¥ ì‹œ handle_query_submission í˜¸ì¶œ)
st.text_input(
    "ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ì„¸ìš”:",
    key="user_input_text",
    on_change=handle_query_submission, # ì—”í„° ì…ë ¥ ì‹œ ì½œë°±
    placeholder="PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”..."
)

# "ì „ì†¡" ë²„íŠ¼ (í´ë¦­ ì‹œ handle_query_submission í˜¸ì¶œ)
# st.buttonì€ í´ë¦­ ì‹œ Trueë¥¼ ë°˜í™˜í•˜ë©°, ì´ë•Œ ì½œë°±ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
if st.button("ì „ì†¡"):
    # ë²„íŠ¼ í´ë¦­ ì‹œ handle_query_submission í˜¸ì¶œ (st.text_inputì˜ í˜„ì¬ ê°’ì„ ì‚¬ìš©)
    handle_query_submission()


# í•˜ë‹¨ í€µ ì§ˆë¬¸ ë²„íŠ¼ (ê¸°ì¡´ ë¡œì§ ìœ ì§€, ì§ì ‘ íˆ´ í˜¸ì¶œ)
col1, col2, col3, col4 = st.columns(4)

with col1:
    if st.button("ğŸ” PDF ê²€ìƒ‰ ì§ˆë¬¸ (ì˜ˆì‹œ)"):
        if st.session_state.index is None:
            st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì¸ë±ì‹±í•´ì•¼ í•©ë‹ˆë‹¤.")
        else:
            query = "ë³´ê³ ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            st.session_state.chat_history.append(("user", query))
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    response = pdf_search_tool(query) # ì§ì ‘ íˆ´ í˜¸ì¶œ (Langgraph í†µí•˜ì§€ ì•ŠìŒ)
                    st.session_state.chat_history.append(("assistant", response))
                except Exception as e:
                    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            st.rerun()
        
with col2:
    if st.button("ğŸŒ ì¼ë°˜ ì§ˆë¬¸ (ì˜ˆì‹œ)"):
        query = "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”?"
        st.session_state.chat_history.append(("user", query))
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                response = general_chat_tool(query) # ì§ì ‘ íˆ´ í˜¸ì¶œ
                st.session_state.chat_history.append(("assistant", response))
            except Exception as e:
                st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        st.rerun()
        
with col3:
    if st.button("ğŸ¤– AI ì§ˆë¬¸ (ì˜ˆì‹œ)"):
        query = "ChatGPTì™€ Claudeì˜ ì°¨ì´ì ì€?"
        st.session_state.chat_history.append(("user", query))
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                response = general_chat_tool(query) # ì§ì ‘ íˆ´ í˜¸ì¶œ
                st.session_state.chat_history.append(("assistant", response))
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        st.rerun()
        
with col4:
    if st.button("ğŸ’» ì½”ë”© ì§ˆë¬¸ (ì˜ˆì‹œ)"):
        query = "íŒŒì´ì¬ í´ë˜ìŠ¤ì™€ ê°ì²´ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì¤˜"
        st.session_state.chat_history.append(("user", query))
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                response = general_chat_tool(query) # ì§ì ‘ íˆ´ í˜¸ì¶œ
                st.session_state.chat_history.append(("assistant", response))
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        st.rerun()